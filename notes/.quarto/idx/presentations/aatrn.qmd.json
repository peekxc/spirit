{"title":"Spectral relaxations of persistent rank invariants","markdown":{"yaml":{"title":"Spectral relaxations of persistent rank invariants","author":"Matt Piekenbrock$\\mathrm{}^\\dagger$   \\&   Jose Perea$\\mathrm{}^\\ddagger$","format":{"revealjs":{"default-timing":60,"css":["katex.min.css","styles.css"],"html-math-method":{"method":"katex","url":"/"},"smaller":true,"theme":"simple","institute":["$\\dagger$ Khoury College of Computer Sciences, Northeastern University","$\\ddagger$. Department of Mathematics and Khoury College of Computer Sciences, Northeastern University"],"spotlight":{"useAsPointer":false,"size":55,"toggleSpotlightOnMouseDown":false,"spotlightOnKeyPressAndHold":16,"presentingCursor":"default"},"overview":true,"margin":0.075,"title-slide-attributes":{"data-background-image":"images/NE.jpeg","data-background-size":"contain","data-background-opacity":"0.25"}},"pdf":"default"},"revealjs-plugins":["spotlight"],"html":{"html-math-method":"katex","standalone":true},"filters":["roughnotation"],"bibliography":"../references.bib"},"headingText":"Learning with persistence","containsRefs":true,"markdown":"\n\n\n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n::: {.fragment .fade-in fragment-index=1 style=\"text-align: left\"}\n\n- Persistence Landscapes [@bubenik2020persistence]\n\n:::\n\n\n::: {.fragment .fade-in-then-out fragment-index=1 style=\"text-align: center\"}\n\n![](images/pers_landscape_def.png){width=40% fig-align=\"center\"}\n\n$$ \\lambda(k, t) = \\sup \\{ h \\geq 0 \\mid \\mathrm{rank}(H_p^{i-h} \\to H_p^{i+h}) \\geq k \\} $$\n\n:::\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n- Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay]\n\n![](images/pers_landscape_app.png){width=40% fig-align=\"center\"}\n\n$$ \\lambda(k, t) = \\sup \\{ h \\geq 0 \\mid \\mathrm{rank}(H_p^{i-h} \\to H_p^{i+h}) \\geq k \\} $$\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n<ul> \n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>\n\n:::\n\n::: {.fragment .fade-in fragment-index=1 style=\"text-align: left\"}\n\n<li> Persistence Images [@adams2017persistence] </li> \n\n:::\n\n<ul> \n\n\n::: {.fragment .fade-in-then-out fragment-index=1 style=\"text-align: center\"}\n\n![](images/pers_image_def.png){height=50% fig-align=\"center\"}\n\n$$ \\rho(f, \\phi) = \\sum\\limits_{(i,j) \\in \\mathrm{dgm}} f(i,j) \\phi(\\lvert j - i \\rvert)$$\n\n:::\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n<ul> \n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>\n\n:::\n\n<li> Persistence Images [@adams2017persistence] + Learning applications [@som2020pi]\n</li> \n\n<ul> \n\n![](images/pers_image_app.png){height=50% fig-align=\"center\"}\n\n$$ \\rho(f, \\phi) = \\sum\\limits_{(i,j) \\in \\mathrm{dgm}} f(i,j) \\phi(\\lvert j - i \\rvert)$$\n\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n<ul> \n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>\n\n:::\n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Images [@adams2017persistence] + Learning applications [@som2020pi] </li>\n\n:::\n\n::: {.fragment .fade-in}\n\n<li> A few others...$^1$ </li> \n\n![](images/vec1.png){width=80% height=100% fig-align=\"center\"}\n\n:::\n\n</ul>\n\n:::{.aside}\n\nSee [@bubenik2020persistence] for an overview. \n\n:::\n\n## Many goals in common...\n\n<hr/> \n\n:::: {.columns}\n\n::: {.column width=40% layout-align=\"left\" style=\"margin-left: 2em; margin-top: 1em;\"}\n\n::: {.fragment fragment-index=1}\n\n- Vectorize persistence information\n\n:::\n\n::: {.fragment fragment-index=2}\n\n- Optimize persistence invariants \n\n:::\n\n::: {.fragment fragment-index=3}\n\n- Leverage the stability of persistence\n\n:::\n\n\n::: {.fragment fragment-index=4}\n\n- Connect to other areas of mathematics\n\n:::\n \n:::\n\n::: {.column width=40% layout-align=\"left\"}\n\n:::{.r-stack}\n\n![](images/pers_image.png){.fragment fragment-index=1 width=\"300\" height=\"300\"}\n\n![](images/pers_landscape_app.png){.fragment fragment-index=2 width=\"300\" height=\"300\"}\n\n![](animations/stability.gif){.fragment fragment-index=3 width=\"400\" height=\"300\"}\n\n![](images/lsst.png){.fragment fragment-index=4 width=\"375\" height=\"375\"}\n\n:::\n\n:::\n\n:::: \n\n::: {.fragment style=\"text-align: center\" style=\"font-size: 40px;\"}\n\n<div style=\"text-align: center; font-size: 35px;\" >\n\n__Can we achieve these goals without computing diagrams?__\n\n</div>\n\n\n:::\n\n::: {.fragment style=\"text-align: center\" style=\"font-size: 40px;\"}\n\n(computing $\\mathrm{dgm}_p(X)$ on point cloud inputs $X \\subset \\mathbb{R}^{n \\times d}$ takes $O(n^{3(p+2)})$ for $p \\geq 1$...)\n:::\n\n<!-- ::: {.fragment style=\"text-align: center\"}\n\n$^\\ast$ _avoid the reduction the algorithm_\n\n:::\n\n::: {.fragment style=\"text-align: center\"}\n\n$$\\mathrm{dgm}(K) \\leftrightarrow R = \\partial V $$\n\n::: -->\n\n<!-- :::{.aside}\n\nImage from: https://epfl-lts2.github.io/gspbox-html/doc/graphs/\n\n::: -->\n\n<br> \n\n## This Talk - Spectral Rank Invariants {visibility=\"hidden\"}\n\n:::{.fragment .fade-in style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates the _persistent rank_ function\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n3. \"Matrix-free\" computation in $O(n)$ memory \n4. Variety of applications, e.g. featurization, optimization, metric learning\n\n</div>\n\n:::\n\n:::{.fragment .fade-in style=\"text-align: left\" layout=\"[[48,52]]\" layout-valign=\"top\"}\n\n![](animations/ph_transform.gif){width=100% height=100% fig-align=\"left\"}\n\n![](animations/trace_summary.gif){width=100% height=100% fig-align=\"left\"}\n\n:::\n\n:::{.aside}\n\\(1\\) The Schatten-1 norms of the operators driving the relaxation are differentiable over the positive semi-definite cone \n:::\n\n## This Talk - Spectral Rank Invariants \n\n:::{.fragment .fade-in style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n:::\n\n\n:::{.fragment .fade-in style=\"text-align: center\"}\n\n<div style=\"margin-left:2.5em;\">\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n</div>\n\n<br/> \n\n:::{style=\"text-align: center\"}\n\n![](images/spectral_interp2.png){height=450 fig-align=\"center\"}\n\n:::\n\n:::\n\n<!-- :::{layout=\"[[80, 50]]\" layout-valign=\"bottom\"}\n\n![](images/size_function.png){height=50% fig-align=\"center\"}\n![](images/spectral_interpolation.png){height=50% fig-align=\"center\"}\n\n:::\n::: -->\n\n\n## This Talk - Spectral Rank Invariants \n\n:::{style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n\n</div>\n\n<div style=\"width: 100%; text-align: center;\">\n![](images/stochastic_trace_estimates.png){width=80% fig-align=\"center\"}\n</div>\n\n:::\n\n## This Talk - Spectral Rank Invariants \n\n:::{style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n3. \"Matrix-free\" computation in $\\approx O(n)$ memory \n\n</div>\n\n<br/> \n\n$$ {\\Large v^T (\\partial_1 \\circ \\partial_1^T) v = \\sum\\limits_{i \\sim j}(v_i - v_j) }$$\n\n:::\n\n## This Talk - Spectral Rank Invariants \n\n:::{style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n3. \"Matrix-free\" computation in $\\approx O(n)$ memory \n4. Variety of applications, e.g. featurization, optimization, metric learning\n\n</div>\n\n![](images/overview_blackbox.png){width=90% fig-align=\"center\"}\n\n:::\n\n## Application: optimizing filtrations {visibility=\"hidden\"}\n\n![](images/combined_mult.png){width=68% height=100% fig-align=\"center\"}\n\n$$ \\alpha^\\ast = \\argmax_{\\alpha \\in \\mathbb{R}} \\; \\mathrm{card}\\big(\\, \\left.\\mathrm{dgm}(K_\\bullet, \\, f_\\alpha) \\right|_{R} \\, \\big) $$\n\n## Application: _sifting_ bifiltrations\n\n:::{.fragment}\n\nSuppose you want to infer whether data lies in the vicinity of some topological space\n\n:::\n\n:::{.fragment}\n\n::::{.columns}\n\n:::{.column}\n\n![](images/patch_manifold.png){fig-align=\"right\" style=\"max-width: 350px !important; max-height: 350px !important;\"}\n\n:::\n\n:::{.column}\n\n![](images/five_circle.png){fig-align=\"left\" style=\"max-width: 350px !important; max-height: 350px !important;\"}\n\n:::\n\n::::\n\n:::\n\n:::{.fragment}\n\n<div style=\"text-align: center;\">\n\nPersistence may fail to find evidence of this space due to _strong outliers_\n\n</div>\n\n:::\n\n:::{.fragment}\n\n$${\\large \\text{ Solution: add a (co)-density filter }}$$\n\n:::\n\n## Application: _sifting_ bifiltrations\n\n<div style=\"text-align: center;\">\n\nSuppose we filter the data by _diameter_ and _codensity_\n\n</div>\n\n<!-- ::::{.columns}\n\n:::{.column} -->\n\n![](images/hilbert_unmarked.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n<!-- :::\n\n\n:::{.column}\n\n:::\n\n\n:::: -->\n\n<!-- <div style=\"text-align: center;\">\n\nMulti-parameter persistence can remedy this situation!\n\n</div> -->\n\n\n## Application: _sifting_ bifiltrations\n\n<div style=\"text-align: center;\">\n\nSuppose we filter the data by _diameter_ and _codensity_\n\n</div>\n\n<!-- ::::{.columns}\n\n:::{.column} -->\n\n![](images/hilbert_marked.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n<!-- :::\n\n\n:::{.column}\n\n:::\n\n\n:::: -->\n\n<div style=\"text-align: center;\">\n\n...and we highlight the constant value regions with dimension $5$\n\n</div>\n\n\n\n## Application: _sifting_ bifiltrations\n\n<div style=\"text-align: center;\">\n\nSuppose we restrict the bifiltration to line & compute its 1d persistence\n\n</div>\n\n::::{.columns}\n\n:::{.column} \n\n![](images/hilbert_marked_dim.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n:::\n\n\n:::{.column}\n\n![](images/pers5.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n:::\n\n\n::::\n\n:::{.fragment}\n\n<div style=\"text-align: center;\">\n\nMulti-parameter persistence can remedy this situation!\n\n</div>\n\n:::\n\n## Application: _sifting_ bifiltrations\n\n\n![](images/hilbert_scanning2.png){fig-align=\"center\" style=\"max-width: 1150px !important; max-height: 1150px !important;\"}\n\n## Why the rank invariant {visibility=\"hidden\"}\n\n\n:::{.fragment}\n\nThere is a duality between diagrams its associated rank function:\n\n$$ \\mathrm{dgm}_p(\\, K_\\bullet, \\, f \\, ) \\triangleq \\{ \\, ( \\, i, j \\,) \\in \\Delta_+ :  \\mu_p^{i,j} \\neq 0 \\, \\} \\; \\cup \\; \\Delta $$\n\n$$\\text{where: } \\quad \\mu_p^{i,j} = \\left(\\beta_p^{i,j{\\small -}1} - \\beta_p^{i,j} \\right) - \\left(\\beta_p^{i{\\small -}1,j{\\small -}1} - \\beta_p^{i{\\small -}1,j} \\right) \\quad $$\n\n:::\n\n:::{.fragment}\n\n_Fundamental Lemma of Persistent Homology_ shows diagrams characterize their ranks\n$$\\beta_p^{k,l} = \\sum\\limits_{i \\leq k} \\sum\\limits_{j > l} \\mu_p^{i,j}$$\n\n:::\n\n:::{.incremental}\n\n- _Persistence measures_ [@chazal2016structure] extend (1,2) naturally when $\\mathbb{F} = \\mathbb{R}$ \n- Stability in context of multidimensional persistence [@cerri2013betti] \n- Generalizations of rank invariant via Möbius inversion [@mccleary2022edit] and via zigzag persistence[@dey2021computing]\n\n:::\n\n## Overview\n\n<ul>\n  <li>Theory</li>\n  <ul>\n    <li>Rank duality</li>\n    <li>Spectral functions</li>\n    <li>Operator properties</li>\n  </ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Applications</p></li>\n  <ul>\n    <li>Variational + geometric perspective</li>\n    <li>Filtration optimization</li>\n    <li>Feature generation</li>\n  </ul>\n  </div>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Computation (time permitting)</p></li>\n  <ul>\n    <li>Implicit trace estimation</li>\n    <li>Stochastic Lanczos quadrature</li>\n    <li>matvecs are all you need</li>\n  </ul>\n  </div>\n</ul>\n\n\n## The rank invariants {style=\"text-align: center;\"}\n\n::::{.columns}\n\n:::{.column}\n\n![](images/dgm_pbn.png){width=325 height=325 fig-align=\"center\"}\n\n<div style=\"text-align: center;\">\n\n$\\beta_p^{i,j}(K)$\n\n</div>\n\n:::\n\n:::{.column}\n\n![](images/dgm_mu.png){width=325 height=325 fig-align=\"center\"}\n\n<div style=\"text-align: center;\">\n\n$\\mu_p^R(K)$\n\n</div>\n\n:::\n\n::::\n\n## \n\n\n![](images/function_persistence.png){width=875 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::{.fragment}\n$$ \\mathrm{dgm}_p(K, f) \\triangleq \\{ \\, (a, b) :  \\mu_p^{a,b} \\neq 0  \\, \\} $$\n\n$$ \\mu_p^R(K, f) = \\beta_{p}^{b,c} - \\beta_{p}^{a,c} - \\beta_{p}^{b,d} + \\beta_{p}^{a,d}, $$\n\n:::\n\n\n<!-- $$\\mu_p^R(K, f) \\triangleq \\mathrm{card}\\left(\\mathrm{dgm}_p(K, f) \\mid_R \\right) = \\beta_{p}^{b,c} - \\beta_{p}^{a,c} - \\beta_{p}^{b,d} + \\beta_{p}^{a,d}$$  -->\n\n\n##\n\n![](images/size_function2.png){width=875 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n$$\n\\begin{align*}\n\\beta_p : \\; & \\;\\; \\Delta_+ & \\to & \\quad \\mathbb{Z}_+ &&  \\\\\n& (a,b) & \\mapsto & \\quad \\mathrm{rank}(H_p(K_a) \\to H_p(K_b)) && \n\\end{align*}\n$$\n\n\n## The rank invariant {style=\"text-align: center;\"}\n\n__Goal:__ \"Relax\" this integer-valued function via _spectral_ characterization of rank:\n\n![](images/spectral_interpolation_3.png){width=975 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::{.fragment .fade-in-then-semi-out}\n\n$$ \n\\begin{equation}\n\\mu_p^{a, b} \\triangleq \\; \\min_{\\delta > 0} \\left(\\beta_p^{a + \\delta, b  - \\delta} - \\beta_p^{a + \\delta, b  + \\delta} \\right) - \\left(\\beta_p^{a- \\delta, b - \\delta} - \\beta_p^{a - \\delta, b + \\delta} \\right) \n\\end{equation} \n$$\n\n:::\n\n\n:::{.fragment}\n$$ \n\\begin{equation}\n\\beta_p^{a,b} = \\mathrm{rank}(H_p(K_a) \\to H_p(K_b)) \\quad \\Leftrightarrow \\quad \\mathrm{rank}(X) = \\sum_{i=1}^n \\, \\mathrm{sgn}_+(\\sigma_i)\n\\end{equation} \n$$\n:::\n\n<!-- \n## Restrictions & Implications\n\n\n:::{style=\"text-size: 14px; text-align: center; margin-top: 1em;\"}\n\n\n1. Field coefficients in $\\mathbb{R}$<sup>1</sup>\n2. Filtrations ordered simplexwise _reverse colexicographically_\n\n:::\n\n<br/> \n\n$$\n\t{\\color{green} \\mu_p^{R}} = \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{j + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{i + 1, l} }\n\t\\end{bmatrix}\n\t- \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{i + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{j + 1, l} }\n\t\\end{bmatrix}\n$$\n\n<br/>\n\n:::{style=\"text-size: 14px; text-align: center;\"}\n\n:::{.fragment}\n\nThere are advantages to preferring _this_ expression for $\\mu_p^R$\n\n:::\n\n<ol>\n:::{.fragment}\n  <li> <span style=\"color: blue;\"> Inner terms </span> are _unfactored_ </li>\n:::\n:::{.fragment}\n  <li> Variational perspectives on <span style=\"color: red;\">rank function </span> well-studied ($\\mathbb{R}$)</li>\n:::\n:::{.fragment}\n  <li> Theory of <span style=\"color: green;\">_persistent measures_$^{\\ast}$</span> readily applicable </li>\n:::\n</ol>\n\n:::\n\n::: aside \n\n1. Or with a zero-characteristic field.\n::: -->\n\n\n## Key technical observation {visibility=\"hidden\"}\n\n:::{.fragment style=\"text-align: center;\"}\n\nPairing uniqueness lemma can be used to show: \n\n$$ \n\\text{ if } (i,j) \\in \\mathrm{dgm}(K_\\bullet), \\text{ and } R = \\partial V\n$$\n \n:::\n\n:::{.fragment style=\"margin: 0; padding: 0;\"}\n\n$$\n\\implies \\mathrm{rank}(R^{i,j}) - \\mathrm{rank}(R^{i\\texttt{+}1,j}) + \\mathrm{rank}(R^{i\\texttt{+}1,j\\text{-}1}) - \\mathrm{rank}(R^{i,j\\text{-}1}) \\neq 0\n$$\n\n![](images/rank_ll.png){width=575 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::\n\n:::{.fragment}\n\n$$\n\\Rightarrow \\mathrm{rank}(R^{i,j}) = \\mathrm{rank}(\\partial^{i,j}) \n$$\n\n:::\n\n## Key technical observation\n\n:::{.fragment }\n\n![](images/rv_ll.png){width=950 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\n__Pairing uniqueness lemma__<sup>1</sup>  $\\implies \\; \\mathrm{rank}(R^{i,j}) = \\mathrm{rank}(\\partial^{i,j})$\n\n:::\n\n<br/> \n\n:::{.fragment style=\"text-align: center; font-size: 38px;\"}\n\n$\\Leftrightarrow$ Can deduce $\\mathrm{dgm}$'s from ranks of \"lower-left\" blocks of $\\partial_p(K_\\bullet)$\n\n:::\n\n\n:::aside\n\n1. Cohen-Steiner, David, Herbert Edelsbrunner, and Dmitriy Morozov. \"Vines and vineyards by updating persistence in linear time.\" Proceedings of the twenty-second annual symposium on Computational geometry. 2006.\n\n:::\n\n## Key technical observation\n\n$$ \n\\begin{equation}\n\\mathrm{rank}(R^{i,j}) = \\mathrm{rank}(\\partial^{i,j})  \n\\end{equation}\n$$\n \n<hr>\n\n<!-- :::{.fragment style=\"text-align: center;\"}\n\n$(1)$ often used to show correctness of reduction, but far more general:\n\n::: -->\n\n:::{.fragment}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 2em; \">\n__Corollary [@bauer2022keeping]__: &nbsp;Any algorithm that preserves the ranks of the submatrices $\\partial^{i,j}$ for all $i,j \\in \\{ 1, \\dots, n \\}$ is a valid barcode algorithm.\n</div>\n\n:::\n\n:::{.fragment}\n$$ \n\\begin{equation}\n(1) \\Rightarrow \\beta_p^{i,j} = \\lvert C_p(K_i) \\rvert - \\mathrm{rank}(\\partial_p^{1,i}) - \\mathrm{rank}(\\partial_{p+1 }^{1,j}) + \\mathrm{rank}(\\partial_{p+1}^{i + 1, j} ) \n\\end{equation}\n$$\n\n:::\n\n\n:::{.fragment}\n\n$$ \n\\begin{equation}\n(2) \\Rightarrow \\mu_p^{R} = \\mathrm{rank}(\\partial_{p+1}^{j + 1, k})  - \\mathrm{rank}(\\partial_{p+1}^{i + 1, k})  - \\mathrm{rank}(\\partial_{p+1}^{j + 1, l}) + \\mathrm{rank}(\\partial_{p+1}^{i + 1, l})  \n\\end{equation}\n$$\n\n:::\n\n:::{.aside}\n\n@edelsbrunner2000topological noted (1) in passing showing correctness of reduction; @dey2022computational explicitly prove (2); (3) was used by @chen2011output. (2) & (3) are connected to relative homology.\n\n:::\n\n\n## Restrictions & Implications\n\n\n:::{style=\"text-size: 14px; text-align: center; margin-top: 1em;\"}\n\nWe restrict<sup>1</sup> to persistence with field coefficients in $\\mathbb{R}$\n\n:::\n\n<br/> \n\n$$\n\t{\\color{green} \\mu_p^{R}} = \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{j + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{i + 1, l} }\n\t\\end{bmatrix}\n\t- \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{i + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{j + 1, l} }\n\t\\end{bmatrix}\n$$\n\n<br/>\n\n:::{style=\"text-size: 14px; text-align: center;\"}\n\n:::{.fragment}\n\nThere are advantages to preferring _this_ expression for $\\mu_p^R$\n\n:::\n\n<ol>\n:::{.fragment}\n  <li> <span style=\"color: blue;\"> Inner terms </span> are _unfactored_ </li>\n:::\n:::{.fragment}\n  <li> Variational perspectives on <span style=\"color: red;\">rank function </span> well-studied ($\\mathbb{R}$)</li>\n:::\n:::{.fragment}\n  <li> Theory of <span style=\"color: green;\">_persistent measures_<sup>2</sup></span> readily applicable </li>\n:::\n</ol>\n\n:::\n\n::: aside \n\n1. Or any zero characteristic field\n2. Chazal, Frédéric, Vin De Silva, Marc Glisse, and Steve Oudot. 2016. The Structure and Stability of Persistence Modules.\n\n:::\n\n\n\n## Restrictions & Implications\n\n\n:::{style=\"text-size: 14px; text-align: center; margin-top: 1em;\"}\n\nWe restrict<sup>1</sup> to persistence with field coefficients in $\\mathbb{R}$\n\n:::\n\n<br/> \n\n$$\n\t{\\color{green} \\mu_p^{R}} = \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\mathcal{L}_{p}^{j + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\mathcal{L}_{p}^{i + 1, l} }\n\t\\end{bmatrix}\n\t- \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\mathcal{L}_{p}^{i + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\mathcal{L}_{p}^{j + 1, l} }\n\t\\end{bmatrix}\n$$\n\n<br/>\n\n:::{style=\"text-size: 14px; text-align: center;\"}\n\nThere are advantages to preferring _this_ expression for $\\mu_p^R$\n\n<ol>\n  <li> <span style=\"color: blue;\"> Inner terms </span> are _unfactored_ </li>\n  <li> Variational perspectives on <span style=\"color: red;\">rank function </span> well-studied ($\\mathbb{R}$)</li>\n  <li> Theory of <span style=\"color: green;\">_persistent measures_<sup>2</sup></span> readily applicable </li>\n</ol>\n\n:::\n\n::: aside \n\n1. Or any zero characteristic field\n2. Chazal, Frédéric, Vin De Silva, Marc Glisse, and Steve Oudot. 2016. The Structure and Stability of Persistence Modules.\n\n:::\n\n## Relaxing the rank function {visibility=\"hidden\"}\n\n:::{style=\"text-align: center\"} \n\n__Spectral relaxation__: Approximate $\\mathrm{rank}$ with _matrix functions_ [@bhatia2013matrix]\n\n:::\n\n\n:::{style=\"list-style-type: none; align=center;\"}\n\n<div class=\"columns\">\n\n<div class=\"column\">\n\n::: {.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: left\"}\n\n$\\quad\\quad\\quad\\quad \\mathrm{rank}(X) = \\sum \\, \\mathrm{sgn}_+(\\sigma_i)$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=2 style=\"text-align: left\"}\n\n$\\quad\\quad\\quad\\quad \\hphantom{\\mathrm{rank}(X)}\\approx \\sum\\limits_{i=1}^n \\, \\phi(\\sigma_i, \\tau) \\phantom{\\int\\limits_{-\\infty}^x}$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=3 style=\"text-align: left\"}\n\n$\\quad\\quad\\quad\\quad \\hphantom{\\mathrm{rank}(X)}=\\lVert \\Phi_\\tau(X) \\rVert_\\ast$\n\n:::\n\n</div>\n\n<div class=\"column\">\n\n::: {.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: right\"}\n\nwhere $\\quad\\quad$ $X = U \\mathrm{Diag}(\\mathbf{\\sigma})V^T$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=2 style=\"text-align: right\"}\n\nwhere $\\quad \\phi(x, \\tau) \\triangleq \\int\\limits_{-\\infty}^x\\hat{\\delta}(z, \\tau) dz$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=3 style=\"text-align: right\"}\n\nwhere $\\quad \\Phi_\\tau(X) \\triangleq \\sum_{i=1}^n \\phi(\\sigma_i, \\tau) u_i v_i^T$\n\n:::\n\n</div>\n\n</div>\n\n::: {.fragment .fade-in-then-semi-outstyle=\"text-align: center\"}\n\n$\\Phi_\\tau(X)$ is a _Löwner operator_ when $\\phi$ is _operator monotone_ [@jiang2018unified]\n\n$$ A \\succeq B \\implies \\Phi_\\tau(A) \\succeq \\Phi_\\tau(B) $$\n\n:::\n\n:::{.fragment style=\"text-align: center\"}\n\nClosed-form proximal operators exist when $\\Phi_\\tau$ convex [@beck2017first]\n\nOften used in nonexpansive mappings [@bauschke2011convex]\n\n:::\n\n:::\n\n## Relaxing the rank function\n\n<hr/> \n\n:::{.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: center\"} \n\n__Spectral relaxation__: Approximate $\\mathrm{rank}$ by approximating the _sign_ function \n\n:::\n\n\n:::{style=\"list-style-type: none; align=center;\"}\n\n::: {.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: center\"}\n\n$$\\mathrm{rank}(X) = \\mathrm{rank}(X X^T) = \\sum_{i=1}^n \\, \\mathrm{sgn}_+(\\lambda_i) \\approx \\sum_{i=1}^n \\, \\phi(\\lambda_i, \\tau) $$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=2 style=\"text-align: left\"}\n\n$$\\text{ where } \\phi(x, \\tau) \\triangleq \\int\\limits_{-\\infty}^x\\hat{\\delta}(z, \\tau) dz \\text{ for a smoothed Dirac measure } \\hat{\\delta}^1$$\n\n<!-- \\hat{\\delta}(x, \\tau) = \\frac{1}{\\nu(\\tau)} p\\left(\\frac{x}{\\nu(\\tau)}\\right), \\quad \\tau > 0, \\quad \\nu \\text{ inc. } -->\n:::\n\n\n::: {.fragment .fade-in-then-semi-out fragment-index=3 style=\"text-align: center\"}\n\n$\\phi : \\mathbb{R} \\to \\mathbb{R}$ induces a unique$^2$ _spectral function_ $F: S_{n} \\to \\mathbb{R}$ via its _trace_: \n\n$$\\mathrm{tr}(\\Phi_\\tau(X)) = \\sum\\limits_{i=1}^n \\phi(\\lambda_i, \\tau), \\quad \\Phi_\\tau(X) \\triangleq U \\phi_\\tau(\\Lambda) U^T $$\n\n:::\n\n\n<!-- ::: {.fragment .fade-in-then-semi-out fragment-index=4 style=\"text-align: center\"}\n\nThe operator $\\Phi_\\tau(X) \\in \\mathbb{R}^{n \\times n}$ is called a _Löwner operator_ [@bhatia2013matrix]\n\n::: -->\n\n\n<!-- ::: {.fragment .fade-in-then-semi-out fragment-index=4 style=\"text-align: center\"}\n\n$\\Phi_\\tau(X)$ is a _Löwner operator_ when $\\phi$ is _operator monotone_ [@jiang2018unified]\n\n$$ A \\succeq B \\implies \\phi_\\tau(A) \\succeq \\phi_\\tau(B) $$\n\n:::\n\n:::{.fragment style=\"text-align: center\"}\n\nClosed-form proximal operators exist when $\\phi_\\tau$ convex + minor conditions$^1$ \n\n::: -->\n\n:::\n\n<aside class=\"aside\" style=\"text-align: center !important;\">\n\n\\(1\\) Any $\\hat{\\delta}$ of the form $\\nu(1/\\tau) p (z \\cdot \\nu (1/\\tau))$ where $p$ is a density function and $\\nu$ positive and increasing is sufficient.\n\n\\(2\\) See Theorem 1.2 of @jiang2018unified for uniqueness conditions.\n<!-- \\(1\\) See @beck2017first and @bauschke2011convex for existence and optimality conditions.  -->\n\n</aside>\n\n\n## Spectral functions\n\nFor any smoothed Dirac measure^[Any $\\hat{\\delta}$ of the form $\\nu(1/\\tau) p (z \\cdot \\nu (1/\\tau))$ where $p$ is a density function and $\\nu$ positive and increasing is sufficient.] $\\hat{\\delta}$ and _operator monotone_ $\\phi: \\mathbb{R}_+ \\times \\mathbb{R}_{++} \\to \\mathbb{R}_+$, @bi2013approximation show that:\n\n<!-- <div class=\"columns\" style=\"margin-left: 2.5em; \"> -->\n\n<div style=\"list-style-type: none !important;\">\n\n<div class=\"columns\">\n\n<div class=\"column\" style=\"width: 30%\">\n\n:::{.fragment .fade-in fragment-index=1 .no_bullet}\n\n($\\tau$-approximate) $\\vphantom{\\hat{\\delta}}$\n\n:::\n\n:::{.fragment .fade-in fragment-index=2}\n\n(Monotone) $\\vphantom{\\lVert \\phi_{\\tau}(X) \\rVert_\\ast}$\n\n:::\n\n:::{.fragment .fade-in fragment-index=3}\n\n(Smooth) $\\vphantom{\\mathbb{R}_1^{n \\times m^{\\ast^{\\ast}}}}$\n\n:::\n\n:::{.fragment .fade-in fragment-index=4}\n\n(Explicit) $\\vphantom{\\partial \\lVert \\Phi_\\tau(\\cdot) \\rVert_\\ast}$ \n\n:::\n\n</div>\n\n<div class=\"column\" width=\"70%\" layout-align=\"right\">\n\n:::{.fragment .fade-in fragment-index=1}\n\n$0 \\leq \\mathrm{rank}(X) - \\lVert \\Phi_\\tau(X) \\rVert_\\ast \\leq c(\\hat{\\delta})$\n\n:::\n\n:::{.fragment .fade-in fragment-index=2}\n\n$\\lVert \\Phi_{\\tau}(X) \\rVert_\\ast \\geq \\lVert \\Phi_{\\tau'}(X) \\rVert_\\ast$ for any $\\tau \\leq \\tau'$\n\n:::\n\n:::{.fragment .fade-in fragment-index=3}\n\nSemismooth^[Here _semismooth_ refers to the existence of directional derivatives] on $\\mathbb{R}^{n \\times m}$ $\\vphantom{\\mathbb{R}_1^{n \\times m^{\\ast^{\\ast}}}}$, differentiable on $\\mathbf{S}_+^m$\n\n:::\n\n:::{.fragment .fade-in fragment-index=4}\n\nDifferential $\\partial \\lVert \\Phi_\\tau(\\cdot) \\rVert_\\ast$ has closed-form soln.\n\n:::\n\n</div>\n\n</div>\n\n:::{.fragment .fade-in fragment-index=5}\n\nFunction/operator pairs ( $\\phi_\\tau$, $\\Phi_\\tau$ ) particular specializations of _matrix functions_:\n\n$$\\Phi_\\tau(X) = U \\phi_\\tau(\\Lambda) U^T$$\n\nCommonly used in many application areas, e.g. compressed sensing [@li2014new]\n\n:::\n\n</div>\n\n## Combinatorial Laplacian {visibility=\"hidden\"}\n\n__Relax #3:__ Replace $\\partial \\mapsto L$ with _combinatorial Laplacians_ [@horak2013spectra]:\n\n$$ \\Delta_p = \\underbrace{\\partial_{p+1} \\partial_{p+1}^T}_{L_p^{\\mathrm{up}}}  + \\underbrace{\\partial_{p}^T \\partial_{p}}_{L_p^{\\mathrm{dn}}} $$\n\n:::{.fragment}\n\n$f_\\alpha$ is 1-to-1 correspondence with inner products on cochain groups $C^p(K, \\mathbb{R})$ \n\n$$L_p^{i,j}(\\alpha) \\Leftrightarrow \\langle \\; f,\\, g \\; \\rangle_{\\alpha} \\; \\text{ on } \\;  C^{p+1}(K, \\mathbb{R})$$\n\n::: \n\n:::{.fragment}\n\nBenefits: Symmetric, positive semi-definite, have \"nice\" linear and quadratic forms:\n$$\nL_p^{\\text{up}}(\\tau, \\tau')= \\begin{cases}\n\t\t \\mathrm{deg}_f(\\tau) \\cdot f^{+/2}(\\tau) & \\text{ if } \\tau = \\tau' \\\\ \n%\t\t\\mathrm{deg}(\\tau_i) & \\text{ if } i = j \\\\ \n\t\ts_{\\tau, \\tau'} \\cdot  f^{+/2}(\\tau) \\cdot f(\\sigma) \\cdot f^{+/2}(\\tau') & \\text{ if } \\tau \\overset{\\sigma}{\\sim} \\tau' \\\\\n\t\t0 & \\text{ otherwise} \n\t\\end{cases}\n$$\n\n$\\implies$ can represent operator in \"matrix-free\" fashion\n:::\n\n## Parameterized filtrations {visibility=\"hidden\"}\n\nSuppose we have an $\\alpha$-parameterized filtration $(K, f_\\alpha)$ where $f_\\alpha : K \\to \\mathbb{R}_+$ satisfies:\n\n$$\nf_\\alpha(\\tau) \\leq f_\\alpha(\\sigma) \\quad \\text{ if } \\tau \\subseteq \\sigma \\quad \\forall \\tau,\\sigma \\in K \\text{ and } \\alpha \\in \\mathbb{R}\n$$\n\n:::{layout-ncol=2}\n\n![](animations/codensity_family.gif){width=48% height=100% fig-align=\"right\"}\n\n![](animations/complex_plain.gif){width=48% height=100% fig-align=\"left\"}\n\n:::\n\n## __Relax \\#1__: Parameterized _boundary matrices_ {visibility=\"hidden\"}\n\n:::{.fragment style=\"text-align: center;\"}\n\nParameterize $C_p(K; \\mathbb{R})$ with $\\mathcal{S} \\circ f_\\alpha : K \\to \\mathbb{R}_+$ where $\\mathcal{S}: \\mathbb{R} \\to [0,1]$  \n\n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\n![](images/smoothstep_3.jpeg){width=88% height=100% fig-align=\"center\"}\n\n:::\n\n\n:::{.fragment style=\"text-align: center; border: 1;\"}\n\n$$ \n\\boxed{\n\\partial_p^{i,j}(\\alpha) = D_p(\\mathcal{S}_i \\circ f_\\alpha) \\circ \\partial_p(K_\\preceq) \\circ D_{p+1}(\\mathcal{S}_j \\circ f_\\alpha) \n}\n$$ \n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\n__Note__: $P^T \\partial_p^{i,j}(\\alpha) P$ has rank $= \\mathrm{rank}(R_p^{i,j}(\\alpha))$ for all $\\alpha \\in \\mathbb{R}$. \n\n:::\n\n:::{.aside style=\"text-align: center;\"}\nReplacing $S \\mapsto \\mathcal{S}$ ensures continuity of $\\partial_p^{i,j}(\\alpha)$\n:::\n\n\n## Rank Invariances when $\\mathbb{F} = \\mathbb{R}$ {visibility=\"hidden\"}\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n&emsp;&emsp;&emsp;&emsp;\n\n$\\hspace{10em} \\mathrm{rank}(A) \\triangleq \\mathrm{dim}(\\mathrm{Im}(A))$\n\n::: \n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(A^T) \\quad \\quad  \\quad \\text{(adjoint)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(A^T A) \\quad \\quad \\; \\text{(inner product)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(A A^T) \\quad \\quad \\; \\text{(outer product)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(S^{-1}AS) \\quad \\;  \\text{(change of basis)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(P^T A P) \\quad \\; \\text{(permutation)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\dots  \\quad \\quad \\quad \\quad  \\quad \\quad  \\! \\! \\text{(many others)}$\n\n:::\n\n<br> \n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n<div style=\"text-align: center; font-size: 35px;\" >\n\n__Q: Can we exploit some of these to speed up the computation?__\n\n</div>\n\n:::\n\n## Overview\n\n<ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li>Theory</li>\n  <ul>\n    <li>Rank duality</li>\n    <li>Spectral functions</li>\n    <li>Operator properties</li>\n  </ul>\n  </div>\n  <li><p>Applications</p></li>\n  <ul>\n    <li>Variational + geometric perspective</li>\n    <li>Filtration optimization</li>\n    <li>Feature generation</li>\n  </ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Computation (time permitting)</p></li>\n  <ul>\n    <li>Implicit trace estimation</li>\n    <li>Stochastic Lanczos quadrature</li>\n    <li>matvecs are all you need</li>\n  </ul>\n  </div>\n</ul>\n\n\n## Interpretation: Regularization\n\n::: {.fragment fragment-index=1}\n\nIll-posed linear systems $Ax = b$ are often solved by \"regularized\" least-squares: \n\n$$\nx_\\tau^\\ast = \\argmin\\limits_{x \\in \\mathbb{R}^n} \\lVert Ax - b\\rVert^2 + \\tau \\lVert x \\rVert_1 \n$$\n\n:::\n\n::: {.fragment fragment-index=2}\n\nThe minimizer is given in closed-form by the regularized pseudo-inverse:\n\n$$\nx_\\tau^\\ast = (A^T A + \\tau I)^{-1} A^T b\n$$\n:::\n\n::: {.fragment fragment-index=3}\n\n![](images/lasso.png){width=50% fig-align=\"center\"}\n\n:::\n\n::: aside\n\nImage from: https://thaddeus-segura.com/lasso-ridge/\n\n::: \n\n## Interpretation: Regularization\n\n<br/>\n\n::: {.fragment fragment-index=1}\n\n\n<div style=\"text-align: center;\">\nUnder the appropriate parameters$^1$ for $\\nu$ and $p$, $\\phi$ takes the form:\n\n</div>\n\n$$\n\\phi(x, \\tau) = \\frac{2}{\\tau}\\int\\limits_{0}^z z \\cdot  \\big((z/\\sqrt{\\tau})^2+1\\big)^{-2} dz = \\frac{x^2}{x^2 + \\tau}\n$$\n\n:::\n\n\n::: {.fragment fragment-index=2}\n\n<div style=\"text-align: center;\">\n\nThe corresponding Löwner operator and its Schatten $1$-norm is given$^2$ by:\n\n</div>\n\n$$\n\\Phi_\\tau(X) = (X^T X + \\tau \\, I_n)^{-1} X^T X, \\quad \\quad \\lVert \\Phi_\\tau(X) \\rVert_\\ast = \\sum\\limits_{i = 1}^n \\frac{\\sigma_i(X)^2}{\\sigma_i(X)^2 + \\tau}\n$$\n\n:::\n\n::: {.fragment fragment-index=3}\n\n<div style=\"text-align: center;\">\n\nThis the <span style=\"color: purple;\"> _Tikhonov regularization_ </span> in standard form used in $\\ell_1$-regression (LASSO)\n\n</div>\n\n:::\n\n::: {.fragment fragment-index=4}\n\n<div style=\"text-align: center;\">\n\n$\\Leftrightarrow$ $\\tilde{\\beta}_p$ is a \"Tikhonov-regularized Betti number\"\n\n</div>\n\n:::\n\n::: aside \n\n\\(1\\) This $\\phi$ corresponds to setting $\\nu(\\tau) = \\sqrt{\\tau}$ and $p(x) = 2x (x^2 + 1)^{-2}$; \\(2\\) See Theorem 2 in @zhao2012approximation. \n\n::: \n\n\n## Application \\#1: Filtration optimization\n\n:::{layout=\"[[25,50,25]]\" layout-valign=\"bottom\"}\n\n<div class=\"column\" layout-align=\"right\">\n![](animations/dgms_vineyards.gif){width=100%}\n</div>\n\n<div class=\"column\" layout-align=\"center\">\n![](images/dgm_opt.png){height=100% fig-align=\"center\"}\n</div>\n\n<div class=\"column\" layout-align=\"left\">\n![](animations/complex_plain.gif){width=100%}\n</div>\n\n::::\n\n$$ \n\\alpha^\\ast = \\argmax_{\\alpha \\in \\mathbb{R}} \\; \\mathrm{card}\\big(\\, \\left.\\mathrm{dgm}(K_\\bullet, \\, f_\\alpha) \\right|_{R} \\, \\big) \n$$\n\n## Application \\#1: Filtration optimization\n\n:::{layout=\"[[60,30]]\" layout-valign=\"bottom\"}\n\n<div class=\"column\" layout-align=\"right\">\n![](images/codensity_mult.png)\n</div>\n\n<div class=\"column\" layout-align=\"center\">\n![](images/optimal_codensity_complex.png){width=100% height=100%}\n</div>\n\n:::\n\n$$ \n\\alpha^\\ast = \\argmax_{\\alpha \\in \\mathbb{R}} \\; \\mathrm{card}\\big(\\, \\left.\\mathrm{dgm}(K_\\bullet, \\, f_\\alpha) \\right|_{R} \\, \\big) \n$$\n\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex1.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \n\t\\mu_p^{R} = \n\t\\mathrm{rank}\\begin{bmatrix} \\partial_{p+1}^{j + 1, k} & 0 \\\\\n\t0 & \\partial_{p+1}^{i + 1, l}\n\t\\end{bmatrix}\n\t- \n\t\\mathrm{rank}\\begin{bmatrix} \\partial_{p+1}^{i + 1, k} & 0 \\\\\n\t0 & \\partial_{p+1}^{j + 1, l}\n\t\\end{bmatrix}\n$$\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex2.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \n\\mu_p^{R} = \n\\mathrm{tr}\\begin{bmatrix} \\lVert \\partial_{p+1}^{j + 1, k} \\rVert_\\ast & 0 \\\\\n0 & \\lVert \\partial_{p+1}^{i + 1, l} \\rVert_{\\ast}\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\lVert \\partial_{p+1}^{i + 1, k} \\rVert_\\ast & 0 \\\\\n0 & \\lVert  \\partial_{p+1}^{j + 1, l} \\rVert_\\ast\n\\end{bmatrix}\n$$\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex3.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \n\\hat{\\mu}_p^{R} = \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{j + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{i + 1, l})\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{i + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{j + 1, l})\n\\end{bmatrix}\n$$\n\n:::{.fragment }\n\n$$\\boxed{\\text{There exists a positive }\\tau^\\ast > 0 \\text{ such that } \\mu_p^R = \\lceil \\hat{\\mu}_p^R \\rceil \\text{ for all } \\tau \\in (0, \\tau^\\ast]}$$\n\n:::\n\n\n## Application \\#1: Filtration optimization {visibility=\"hidden\"}\n\n![](images/combinatorial_explosion.png){width=60%, fig-align=\"center\"}[^1]\n\n\n[^1]: Xu, Weiyu, and Babak Hassibi. \"Precise Stability Phase Transitions for $\\ell_1 $ Minimization: A Unified Geometric Framework.\" IEEE transactions on information theory (2011)\n\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex4.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \\mu_p^{R} = \\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{j + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{i + 1, l})\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{i + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{j + 1, l})\n\\end{bmatrix}\n$$\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex5.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \\mu_p^{R} = \\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{j + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{i + 1, l})\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{i + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{j + 1, l})\n\\end{bmatrix}\n$$\n\n:::{.fragment style=\"text-align: center\"}\n\nSimilar to the Iterative Soft-Thresholding Algorithm (ISTA) [@beck2017first]\n\n:::\n\n\n## Interpretation: Diffusion\n\n<hr/>\n\n::: {.fragment fragment-index=1 style=\"text-align: center\"}\n\nDiffusion processes on graphs often modeled as time-varying $v(t) \\in \\mathbb{R}^n$ via:\n\n$$ v'(t) = -L v(t) \\quad \\Leftrightarrow \\quad L \\cdot u(x,t) = - \\frac{\\partial u(x, t)}{\\partial t} $$\n\n::: \n\n::: {.fragment fragment-index=2 layout-valign=\"center\" style=\"text-align: center\"}\n\n![](images/diffusion2.png){width=90% fig-align=\"center\"}\n\n:::\n\n::: {.fragment fragment-index=3 style=\"text-align: center\"}\n\nValue of $v(t)$ at time $t$ given by the _Laplacian exponential diffusion kernel_:\n\n$$v(t) = \\mathrm{exp}(-t L) v(0)$$\n\n<!-- $$H_t = U \\mathrm{exp}(-t \\Lambda) U' = \\sum\\limits_{i=1}^n e^{-t \\lambda_i} \\, u_i \\, u_i^T$$ -->\n\n:::\n\n:::{.aside style=\"margin-top: 2em !important;\"}\n\nImages from @crane2017heat and @sharma2011topologically\n\n::: \n\n\n## Interpretation: Diffusion\n\n<hr/>\n\n::: {.fragment fragment-index=1}\n\n<div style=\"text-align: center;\">\n\nUnder the appropriate parameters for $\\nu$ and $\\rho$^[This $\\phi$ corresponds to setting $\\nu(\\tau) = \\tau$ and $p(x) = \\mathrm{exp}(-x)$ for $x > 0$ and $p(x) = 0$ otherwise], $\\phi$ takes the form:\n\n</div>\n\n$$\n\\phi(x, \\tau) = 1 - \\mathrm{exp}(- x / \\tau)\n$$\n\n:::\n\n\n::: {.fragment fragment-index=3}\n\n<div style=\"text-align: center;\">\n\nThe corresponding Löwner operator and its Schatten $1$-norm is given by (for $t = \\tau^{-1}$):\n\n</div>\n\n$$\n\\Phi_\\tau(X) \\simeq U \\mathrm{exp}(-t \\Lambda) U^T, \\quad \\mathrm{tr}(\\Phi_\\tau(X)) \\simeq \\sum\\limits_{i = 1}^n \\mathrm{exp}(-t \\cdot \\lambda_i)\n$$\n\n:::\n\n::: {.fragment fragment-index=4}\n\n<div style=\"text-align: center;\">\n\nThis is the <span style=\"color: red;\"> _Heat kernel_ </span> and its Schatten-1 norm is the <span style=\"color: red;\"> _heat kernel trace_ </span>\n\n</div>\n\n:::\n\n<br> \n\n::: {.fragment fragment-index=5}\n\n<div style=\"text-align: center; border: 1px;\">\n\nBoth quantities proven useful in crafting _geometric signatures_$^2$\n\n</div>\n\n:::\n\n::: aside\n\nSee e.g. @sun2009concise, @bronstein2010scale, and @xiao2009graph\n\n:::\n\n## Application \\#2: Featurization\n\n<hr/>\n\n:::{.fragment style=\"text-align: center\"}\n\nConsider filtering a fixed $K$ embedded in $R^d$ by a 1-parameter directions in $S^{d-1}$\n\n$$\nK_\\bullet = K(v)_i = \\{\\, x \\in X \\mid \\langle x, v \\rangle \\leq i  \\,\\}\n$$\n\n:::\n\n:::{.fragment style=\"text-align: center\"}\n\n![](animations/dt_single.gif){width=350 height=100% fig-align=\"center\"}\n\n:::\n\n<!-- ![](images/dt.png){width=400 height=100% fig-align=\"center\"} -->\n\n## Application \\#2: Featurization\n\n<hr/>\n\n$$\nK_\\bullet = K(v)_i = \\{\\, x \\in X \\mid \\langle x, v \\rangle \\leq i  \\,\\}\n$$\n\n![](animations/ph_transform.gif){width=550 height=100% fig-align=\"center\"}\n\n\n:::{.fragment}\n\n$$\\{ \\; \\mathrm{dgm}(v) : v \\in S^{d-1} \\; \\} \\Leftrightarrow \\text{Persistent Homology Transform (PHT)}$$ \n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\nTurner et al.$^1$ show PHT(X) is injective, sparking an inverse theory for persistence!\n\n:::\n\n::: aside \n\n\\(1\\): @turner2014persistent\n\n:::\n\n## Application \\#2: Featurization\n\n<hr/>\n\n:::{.fragment .fade-in-then-semi-out}\n\nInjectivity of the PHT $\\implies$ can impose metric over <span style=\"color: orange;\"> _shape space_ </span> by integrating $d_B$:\n\n$$ \\mathcal{D}_{PHT}(X, Y) \\triangleq \\sum_{p=0}^d \\int_{S^{d-1}} \\operatorname{d}_B\\left(\\mathrm{dgm}_p(X, v) \\right), \\left( \\mathrm{dgm}_p(Y, v) \\right) dv $$\n\n:::\n\n<!-- :::{.fragment layout=[[50,50]]}\n\n![](images/turtle1.png){width=650 height=100% fig-align=\"right\"}\n\n![](images/turtle2.png){width=650 height=100% fig-align=\"left\"}\n\n::: -->\n\n<!-- :::{.fragment .fade-in-then-semi-out}\nTo make $\\mathcal{D}(\\cdot, \\cdot)$ blind to rotations, Turner^[@turner2014persistent] minimize $\\mathcal{D}$ over rotations $\\{R_i\\}_{i=1}^m$:\n\n$$ d_{\\mathrm{PHT}}(X, Y) = \\inf_{i = 1, \\dots, m} \\mathcal{D}(X, R_i(Y)) $$\n\n::: -->\n\n\n:::{.fragment .fade-in style=\"text-align: center;\"}\n\n<!-- When $m = \\lvert V \\rvert$, computing $d_{\\mathrm{PHT}}(X, Y)$ requires:  -->\n\n<ol>\n  <li> Computing $\\mathrm{dgm}_p(\\cdot, v)$ for $\\{X,Y\\}$ over $\\{v_i\\}_{i=1}^m \\subset S^{d-1}$ ( ${\\color{orange} O(m \\cdot N^3)}$ )</li>\n  <li> Minimizing $\\mathcal{D}$ over all $m$ rotations ( ${\\color{red}\\approx O(m^2 \\cdot N^{1.5} \\log N)}$ )$^1$ </li>\n</ol>\n\n:::\n\n<!-- <ol>\n  <li> Computing $\\mathrm{tr}(\\phi_\\tau(\\cdot))$ for $\\{X,Y\\}$ over sufficiently dense $\\mathcal{V} \\subset S^{d-1}$ ($\\approx {\\color{blue} O(m \\cdot N^2)}$)</li>\n  <li> Phase-aligning two _periodic_ signals via FFT ( ${\\color{green} O(m \\log m)}$ ) </li>\n</ol> -->\n\n:::{.fragment .fade-in }\n$${\\Large \\text{ This can be very expensive to do! }}$$\n\n:::\n\n::: aside \n\\(1\\) Assumes $d_B \\sim O(n^{1.5} \\log n)$, following [@kerber2017geometry]. \n:::\n\n## Application \\#2: Featurization \n\n<hr/>\n\nInformativity of Heat Kernel $\\implies$ can impose _pseudo_-metric over <span style=\"color: orange;\"> _shape space_</span>:\n\n$$ \\mathcal{D}_{HK}(X, Y) \\triangleq \\sum_{p=0}^d \\int_{S^{d-1}} \\lVert \\mu_p^R(X, v) - \\mu_p^R(Y, v) \\rVert_2 $$\n\n<!-- $$ \\mathcal{D}(X, Y) \\triangleq \\sum_{p=0}^d \\int_{S^{d-1}} \\operatorname{d}_B\\left(\\mathrm{dgm}_p(X, v) \\right), \\left( \\mathrm{dgm}_p(Y, v) \\right) dv $$ -->\n\n\n<!-- When $m = \\lvert V \\rvert$, computing $d_{\\mathrm{PHT}}(X, Y)$ requires:  -->\n<!-- $\\mathrm{tr}(\\phi_\\tau(\\cdot))$ $\\xcancel{d_{\\mathrm{PHT}}(X, Y)}$ requires:  -->\n\n<!-- <ol color=\"gray;\">\n  <li> Computing $\\mathrm{dgm}_p(\\cdot, v)$ for $\\{X,Y\\}$ over sufficiently dense $\\mathcal{V} \\subset S^{d-1}$ ( ${\\color{orange} O(m \\cdot N^3)}$ )</li>\n  <li style=\"text-style: strikethrough;\"> Minimizing $\\mathcal{D}$ over all $m$ rotations ( ${\\color{red}\\approx O(m^2 \\cdot N^{1.5} \\log N)}$ )$^1$ </li>\n</ol> -->\n\n<!-- When $m = \\lvert V \\rvert$, computing $\\mathrm{tr}(\\phi_\\tau(\\cdot))$ $\\xcancel{d_{\\mathrm{PHT}}(X, Y)}$ requires:  -->\n\n:::{style=\"text-align: center;\"}\n\n<ol>\n  <li> Computing $\\mu_p^R(\\cdot, v)$ for $\\{X,Y\\}$ over $\\{v_i\\}_{i=1}^m \\subset S^{d-1}$ ($\\approx {\\color{blue} O(m \\cdot N^2)}$)</li>\n  <li> Phase-aligning two _periodic_ signals via FFT ( ${\\color{green} O(m \\log m)}$ ) </li>\n</ol>\n\n:::\n\n:::{layout=[[36,62]] style=\"text-align: center;\"}\n\n![](animations/dt_single.gif){height=270px fig-align=\"right\"}\n\n![](animations/trace_summary.gif){height=275px fig-align=\"left\"}\n\n:::\n\n## Application \\#2: Featurization\n\n\n\n![](images/shape_signatures.png){width=850 height=100% fig-align=\"center\"}\n\n<!-- :::{layout=[[50,50]]}\n\n![](images/turtle1.png){width=650 height=100% fig-align=\"right\"}\n\n![](images/bone1.png){width=650 height=100% fig-align=\"left\"}\n\n::: -->\n\n\n## Application \\#3: Topology-guided sparsification \n\n![](images/elephant_sparsify.png){fig-align=\"left\" style=\"max-width: 1150px !important; max-height: 1150px !important;\"}\n\n\n## Overview \n\n<ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li>Theory</li>\n  <ul>\n    <li>Rank duality</li>\n    <li>Spectral functions</li>\n    <li>Operator properties</li>\n  </ul>\n  </div>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Applications</p></li>\n  <ul>\n    <li>Variational + geometric perspective</li>\n    <li>Filtration optimization</li>\n    <li>Feature generation</li>\n  </ul>\n  </div>\n  <li><p>Computation (time permitting)</p></li>\n  <ul>\n    <li>Implicit trace estimation</li>\n    <li>Stochastic Lanczos quadrature</li>\n    <li>matvecs are all you need</li>\n  </ul>\n</ul>\n\n\n\n## Beyond $\\mathrm{dgm}$'s: Revisiting the rank computation {visibility='hidden'}\n \n$$ \\beta_p^{i,j} : \\mathrm{rank}(H_p(K_i) \\to H_p(K_j))$$\n\t\n<!-- <hr style=\"margin: 0; padding: 0;\">  -->\n\n:::{.incremental style=\"list-style-type: none;align=center;\"}\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n$\\;\\quad\\quad\\quad\\beta_p^{i,j} = \\mathrm{dim} \\big( \\;\\mathrm{Ker}(\\partial_p(K_i))\\; / \\;\\mathrm{Im}(\\partial_{p+1}(K_j)) \\; \\big )$\n:::\n\n<!-- <li style=\"text-align: left\" class=\"fragment fade-in-then-semi-out\">\n$\\quad\\quad\\quad\\quad\\beta_p^{i,j} = \\mathrm{dim} \\big( \\;\\mathrm{Ker}(\\partial_p(K_i))\\; / \\;\\mathrm{Im}(\\partial_{p+1}(K_j)) \\; \\big )$\n</li> -->\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n<!-- <li style=\"text-align: left\" class=\"fragment fade-in-then-semi-out\"> -->\n$\\;\\quad\\quad\\quad\\hphantom{\\beta_p^{i,j} }= \\mathrm{dim}\\big(\\; \\mathrm{Ker}(\\partial_p(K_i)) \\; / \\; (\\mathrm{Ker}(\\partial_p(K_i)) \\cap \\mathrm{Im}(\\partial_{p+1}(K_j))) \\; \\big )$\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n$\\;\\quad\\quad\\quad\\hphantom{\\beta_p^{i,j}}=\\color{blue}{\\mathrm{dim}\\big(\\;\\mathrm{Ker}(\\partial_p(K_i)) \\; \\big)} \\; \\color{black}{-} \\; \\color{red}{\\mathrm{dim}\\big( \\; \\mathrm{Ker}(\\partial_p(K_i)) \\cap \\mathrm{Im}(\\partial_{p+1}(K_j))\\;\\; \\big)}$\n:::\n\n::: \n\n<!-- <br>  -->\n::: {.fragment .fade-in-then-semi-out}\nRank-nullity yields the <span style=\"color: blue\">left term</span>: \n$$\n\\mathrm{dim}\\big(\\mathrm{Ker}(\\partial_p(K_i))\\big) = \\lvert C_p(K_i) \\rvert - \\mathrm{dim}(\\mathrm{Im}(\\partial_p(K_i)))\n$$\n:::\n\n:::{.fragment .fade-in-then-semi-out}\n<!-- Computing the <span style=\"color: red\">right term</span> more nuanced:  -->\n\"Relaxing\" the <span style=\"color: red\">right term</span> poses some difficulties:\n:::\n\n:::{.incremental style=\"list-style-type: none; align=center; text-align: left; margin-left: 2.5em; margin: 0; padding: 0;\"}\n- Pseudo-inverse$^1$, projectors$^2$, Neumann's inequality$^3$, etc.\n- PID algorithm$^4$, Reduction algorithm$^5$, Persistent Laplacian$^6$\n:::\n\n:::{.aside}\n@anderson1969series, @ben1967geometry, @ben2015projectors, @zomorodian2004computing, @edelsbrunner2000topological, @memoli2022persistent\n:::\n\n\n## Computation in quadratic time {visibility=\"hidden\"}\n\n<hr/> \n\n<div class=\"incremental\" style=\"list-style-type: none; align=left;\">\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\nComputing $A = U \\Lambda U^T$ for any $A \\in \\mathbf{S}_+^n$ bounded by $\\Theta(n^3)$ time and $\\Theta(n^2)$ space^[Assumes the standard matrix multiplication model for simplicity (i.e.  excludes the Strassen-family)]\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\n<u>However</u>, if $v \\mapsto Av \\approx O(n)$, then $\\Lambda(A)$ obtainable in <span style=\"color: red;\"> $O(n^2)$ time </span> and <span style=\"color: red;\">$O(n)$ space</span>!\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\n__Idea__: For some random $v \\in \\mathbb{R}^n$, expand successive powers of $A$:\n\n$$ \n\\begin{align}\nK_j &= [ v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{j-1}v] && \\\\\nQ_j &= [ q_1, q_2, \\dots, q_j] \\gets \\mathrm{qr}(K_j) && \\\\\nT_j &= Q_j^T A Q_j &&\n\\end{align}\n$$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\nIt turns out that every $A \\in \\mathbf{S}$ expanded this way admits a _three-term recurrence_ \n\n$$ A q_j = \\beta_{j-1} q_{j-1} + \\alpha_j q_j + \\beta_j q_{j+1} $$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\n<div style=\"text-align: center; font-size: 34px;\"> \n\nThis is the renowned *__Lanczos method__* for Krylov subspace expansion\n\n</div>\n\n<!-- <img src=\"images/lanczos_top_10.png\" style=\"position: fixed; top: 50%; left: 50%; transform: translate(-50%, -65%); height: 65vh !important; width: 95%;\"> </img> -->\n\n\n:::\n\n</div>\n\n## Lanczos iteration {visibility=\"hidden\"}\n\n![](animations/lanczos_krylov.gif){width=75% height=100% fig-align=\"center\"}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 0em; \">\n__Theorem [@simon1984analysis]__: Given a symmetric rank-$r$ matrix $A \\in \\mathbb{R}^{n \\times n}$ whose matrix-vector operator $A \\mapsto A x$ requires $O(\\eta)$ time and $O(\\nu)$ space, the Lanczos iteration computes $\\Lambda(A) = \\{ \\lambda_1, \\lambda_2, \\dots, \\lambda_r \\}$ in $O(\\max\\{\\eta, n\\}\\cdot r)$ time and $O(\\max\\{\\nu, n\\})$ space _when executed in exact arithmetic_. \n</div>\n\n## Implicit trace estimation\n\n<!-- Spectral-sum computation reduces to  _implicit matrix trace problem_ -->\n\n> __Implicit trace problem__: Given access to a square $A \\in \\mathbb{R}^{n \\times n}$ via its matrix–vector product operation $v \\mapsto Av$, estimate $\\mathrm{tr}(A) = \\sum_{i=1}^n A_{ii}$\n\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n![](images/black_box_trace.png){width=75% height=100% fig-align=\"center\"}\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nExact solution uses $O(n)$ matvecs $\\mathrm{tr}(A) = \\sum e_i^T A e_i$. \n\n:::\n\n<!-- ::: aside \n\n\\(1\\). @epperly2023xtrace @ghorbani2019investigation\n\n::: -->\n\n\n## _Randomized_ implicit trace estimation\n\n<hr/>\n\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nLet $A = \\mathbb{R}^{n \\times n}$. If $v \\in \\mathbb{R}^n$ a $\\mathrm{r.v.}$ with $\\mathbb{E}[vv^T] = I$, then: \n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: left;\"}\n\n$$ \\mathtt{tr}(A) = \\mathtt{tr}(A \\mathbb{E}[v v^T]) =  \\mathbb{E}[\\mathtt{tr}(Avv^T)] = \\mathbb{E}[\\mathtt{tr}(v^T A v)] = \\mathbb{E}[v^T A v] $$\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n$$\n\\implies \\mathtt{tr}(A) \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i, \\quad \\text{ for } v_i \\sim \\{-1, +1\\}^n\n$$\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 0em; \">\n__Theorem [@hutchinson1989stochastic]__: For any $A \\in S_+^n$, if $n_v \\geq (6/\\epsilon^2) \\log(2/\\eta)$ unit-norm $v \\in \\mathbb{R}^n$ are drawn uniformly from $\\{-1, +1\\}^n$, then $\\forall \\; \\epsilon, \\eta \\in (0,1)$:\n$$ \\mathrm{Pr}\\bigg(\\lvert \\mathrm{tr}_{n_v}(A) - \\mathrm{tr}(A) \\rvert \\leq \\epsilon \\cdot \\mathrm{tr}(A) \\bigg) \\geq 1 - \\eta$$\n\n</div>\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nSuch an estimator $\\mu_{n_v} \\triangleq \\mathrm{mean}(\\{ v_i^T A v_i \\}_{i=1}^{n_v})$ is called a _Girard-Hutchinson_ estimator\n\n:::\n\n## Randomized trace approximation \n\n<hr/> \n\n\n<!-- :::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n$\\Phi_\\tau(X)$ are _trace-class_, satisfy $\\mathtt{tr}(\\Phi_\\tau(X)) = \\sum\\limits_{i=1}^n \\phi(\\lambda_i, \\tau)$\n\n::: -->\n\n::::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nUbaru$^1$ showed $\\mu_{n_v}$ generalizes to _matrix functions_ via __stochastic Lanczos quadrature__\n\n$$ \\mathrm{tr}(\\Phi_\\tau(X)) \\approx \\frac{n}{n_v} \\sum\\limits_{i=1}^{n_v} \\bigg( e_1^T \\, \\Phi_{\\tau}(T_i) \\, e_1 \\bigg ), \\quad \\quad T_i = \\mathrm{Lanczos}(X, v_i) $$\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nThe degree-$k$ Lanczos takes $O(\\eta \\cdot k^2)$ time, where $\\eta$ is the time to do $v \\mapsto Xv$\n<!-- $$ \\hat{\\mu}_p^{R} \\sim T(n^2 \\cdot s l^2) \\approx O(n^2) \\text{ time, where } n \\sim \\lvert K^p \\rvert \\text{ and } l, s \\sim O(1)^{1} $$  -->\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nFor $p$-Laplacians, the complexity of $v \\mapsto L_p v$ is $\\approx O(m)^1$ where $m = \\lvert K_{p+1}\\rvert$\n\n:::\n\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n$\\implies$ can compute $\\epsilon$-approx.$^2$ of $\\mu_p^{\\ast}$ or $\\beta_p^{\\ast}$ with success probability $1 - \\eta$ in: \n\n$$ O((m / \\epsilon^{2}) \\log(\\eta^{-1})) \\text{ time}, \\quad O(m) \\text{ space }$$\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n<hr/>\n\n$${\\Large \\text{ Matvecs are all you need! }}$$\n\n<hr/>\n:::\n\n::: aside \n\n\\(1\\) Assumes $p$ is small, fixed constant. \n\\(2\\) Assumes the Lanczos degree $k$ is constant. In practice, $k \\leq 20$ is typically sufficient.\n\n:::\n\n## Brief history of spectral sum estimation \n\nThis is an active area. Sampling of seminal work, improvements, and major applications:\n\n<div style=\"font-size: 1.425rem; text-align: center;\"> \n\n- \\(1950\\) Lanczos publishes \"method of minimized iterations\" for tridiagonalization\n- \\(1969\\) Golub + Welsch study Gaussian quadrature (GQ) rules \n- \\(1989\\) Hutchinson proposes unbiased estimator for $\\mathrm{tr}(A)$ based on QFs\n- \\(2000\\) Estrada proposes trace-based index quantifying \"degree of folding\" in proteins\n- \\(2009\\) Golub elucides connections between orthogonal polynomials, GQ, and \"moment matching\"\n- \\(2017\\) Ubaru proposes _stochastic Lanczos quadrature_: Hutchinson estimator + Lanczos quadrature\n- \\(2017\\) Musco finds Lanczos in FP is optimal for $v \\mapsto f(A)v$ approx.\n- \\(2019\\) Hessian eigenvalue density found crucial to ImageNet performance (est. w/ trace)\n- \\(2021\\) Hutch++ proposed by Musco, achieves optimal $1/m^2$ variance reduction \n- \\(2021\\) Adaptive variant of Hutch++ given that reduces query complexity\n- \\(2021\\) AdaHessian substantially improves Adam performance on ResNet32 + Cifar10\n- \\(2023\\) Epperly uses exchangability to produce family of minimum-variance $\\mathrm{tr}$ estimators\n</div>\n\n## Scalability: $\\mathrm{matvecs}$'s are all you need\n\n![](images/imate_trace_bench.png){width=750 height=100% fig-align=\"center\"}\n\nFigure taken from the new `imate` package documentation ([`[gh]/ameli/imate`](https://ameli.github.io/imate/index.html))\n\n\n## Application \\#3: Computing $\\mathrm{dgm}$'s\n\n\n:::{layout=\"[[50,50]]\" layout-valign=\"bottom\"}\n\n![](images/divide_conquer_dgm.png){width=400 height=100% fig-align=\"right\"}\n\n![](images/bisection_tree.png){width=400 height=100% fig-align=\"left\"}\n\n:::\n\n:::{.fragment}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 0em; \">\n__Theorem [@chen2011output]__: For a simplicial complex $K$ of size $\\lvert K \\rvert = n$, computing the $\\Gamma$-persistent pairs requires $O(C_{(1-\\delta)\\Gamma}\\mathrm{R}(n) \\log n)$ time and $O(n + \\mathrm{R}(n))$ space, where $R(n)$ ($R_S(n)$) is the time (space) complexity of computing the rank of a $n \\times n$ boundary matrix.\n</div>\n\n:::\n\n\n<!-- ## Conclusion \n\nSpectral relaxation of rank invariant using _matrix functions_ \n\n- Suitable for parameterized families of filtrations\n- Differentiable + amenable for optimization \n- Stable to perturbations in $f_\\alpha$ when $\\tau > 0$ \n- Excellent compute properties. Implementation ongoing. \n- Better optimizer implementation also ongoing.  -->\n\n<!-- Looking for collaborators + ideas! In particular: -->\n\n<!-- :::{.incremental}\n\n- Optimizing parameterized filtrations\n- Differentiating n-parameter families of filtrations\n- Encoding features with Laplacian spectra\n- Sparse minimization problems (compressive sensing)\n- Understanding connections to other areas of math\n\n::: -->\n\n## Application: Manifold detection (time permitting) {visibility=\"hidden\"}\n\n:::{layout=\"[[50,50]]\" layout-valign=\"bottom\"}\n\n![](images/patch_manifold.png){width=90% fig-align=\"right\"}\n\n![](images/klein_hilbert.png){width=90% fig-align=\"left\"}\n\n:::\n\n\n## Other applications (time permitting) {visibility=\"hidden\"}\n\n![](images/dw_chi_comp.png){width=50% height=90% fig-align=\"center\"}\n\n![](images/elephant_sparsify.png){width=80% height=100% fig-align=\"center\"}\n\n\n## Acknowledgements & Advertising\n\n- Explicit proof of (unfactored) $\\beta_p^{\\ast}$ in CT book by Tamal Dey & Yusu Wang \n- Expression + PH algorithm involving $\\mu_p^\\ast$ by Chao Chen & Michael Kerber \n- Insightful developments by [@bauer2022keeping] and [@memoli2022persistent]\n- SLQ due to @ubaru2017fast; implementation available @ [gh/peekxc/primate](https://peekxc.github.io/primate/)\n<hr/>\n\n<div style=\"text-align: center; font-size: 40px;\"> \n\nInfo: [[gh]/peekxc](https://github.com/peekxc) or [mattpiekenbrock.com](https://mattpiekenbrock.com/)\n\nPlug: I'm looking for a job this year!\n\n</div>\n\n<hr/>\n\n<div style=\"text-align: center; font-size: 80px;\"> \n\nThanks for listening! \n\n</div>\n\n## References\n\n::: {#refs}\n:::\n\n<script>\n  window.WebFontConfig = {\n    custom: {\n      families: ['KaTeX_AMS'],\n    },\n  };\n</script>\n\n\n\n## Computation {visibility=\"hidden\"}\n\n:::{.fragment}\n\n- Permutation invariance $\\implies$ can optimize memory access of $\\mathtt{SpMat}$ operation\n\n:::\n\n:::{.fragment}\n\n- Any complex data structure suffices, e.g. tries$^2$, combinadics, etc...\n\n::: \n\n:::{.fragment}\n\n- Iterative Krylov methods / Lanczos dominate solving sparse systems$^2$\n\n::: \n\n:::{.fragment}\n\n- Many laplacian preconditioning methods known [@jambulapati2021ultrasparse]\n\n::: \n\n:::{.fragment}\n\n- Nearly optimal algorithms known for SDD [@stathopoulos2007nearly]\n\n::: \n\n:::{.aside}\n\nSee [@komzsik2003lanczos, @parlett1995we] for an overview of the Lanczos. See [@boissonnat2014simplex] for representing complexes.\n\n:::\n\n\n\n\n## Why not just use diagrams?\n\n<!-- Extending the reduction algorithm to parameterized settings is highly non-trivial -->\n\n:::: {.columns}\n\n:::{.fragment}\n\n::: {.column width=\"45%\" layout-align=\"right\" style=\"margin-left: 1em;\"}\n![](animations/dgms_vineyards.gif){width=90%}\n:::\n\n::: {.column width=\"45%\" height=\"1em\" layout-align=\"left\"}\n![](animations/complex_plain.gif){width=90%}\n:::\n\n:::\n\n::::\n\n:::{style=\"text-align: center;\"}\n\n__Pro:__ Diagrams are stable, well-studied, and information rich.\n\n:::\n\n\n::: {.notes}\nReduction algorithm is a restricted form of gaussian elimination. \n:::\n\n\n## Why not just use diagrams?\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n![](animations/dgms_vineyards.gif){width=90%}\n:::\n\n::: {.column width=\"33%\" height=\"1em\"}\n![](animations/complex.gif){width=90%}\n:::\n\n::: {.column width=\"33%\"}\n![](animations/spy_matrices.gif){width=90%}\n:::\n\n::::\n\n:::{style=\"text-align: center;\"}\n\n__Con:__ Extending the $R = \\partial V$ to parameterized settings is non-trivial\n\n:::\n\n<div style=\"text-align: center;\">\n\nMaintaining the $R = \\partial V$ decomposition \"across time\" $\\implies$ huge memory bottleneck\n\n</div>\n\n:::{.aside}\nFor details, see @piekenbrock2021move or @bauer2022keeping (also @lesnick2015interactive)\n:::\n\n::: {.notes}\nReduction algorithm is a restricted form of gaussian elimination. \n:::\n\n\n\n## Computation {visibility=\"hidden\"}\n\nPreliminary experiments suggest scalability is promising \n\n![](images/watts_strogatz_perf.png)\n\n- $\\approx \\, \\leq 25$ Lanczos vectors to approximate full spectrum at any $\\epsilon > 0$\n- $\\implies O(n)$ memory to obtain $\\lVert \\cdot \\rVert_\\ast$ in $O(n^2)$ time (with small constants!)\n- Previously computed eigenvectors can be re-used for \"warm restarts\"\n\n## Experiment \\#1: Directional Transform  {visibility=\"hidden\"}\n\n![](images/shape_signatures.png){width=80% height=100% fig-align=\"center\"}\n\n:::{.notes}\nLuis mentioned modding out rotations and translations adn scale to compare shapes. We can handle rotations via phase alignment. \n\nSarah mentioned handling orientation.\n\n:::\n\n\n## Permutation Invariance {visibility=\"hidden\"}\n\nConsider the setting where $f_\\alpha : \\mathbb{R} \\to \\mathbb{R}^N$ is an $\\alpha$-parameterized filter function: \n\n$$ \\mu_p^R(\\, f_\\alpha \\, ) = \\{ \\mu_p^R(K_\\bullet^\\alpha) : \\alpha \\in \\mathbb{R} \\}$$\n\nDifficult to compute $R_\\alpha = \\partial_\\alpha V_\\alpha$ for all $\\alpha$ as $K_\\bullet = (K, f_\\alpha)$ is changing constantly...\n\n<!-- #\\mathrm{rank}(\\partial_p(K_\\bullet( \\, f_\\alpha \\,))) -->\n\n<!-- On the other hand... -->\n$$ \\mathrm{rank}(\\partial_p(K_\\bullet)) \\equiv \\mathrm{rank}(P^T \\partial_p(K) P) $$\n$$ \\mathrm{rank}(\\partial_p(K_\\bullet)) \\equiv \\mathrm{rank}(W \\mathrm{sgn}(\\partial_p(K)) W) $$\n\nThus we may decouple $f_\\alpha$ and $K$ in the computation: \n\n$$\n\\begin{align*}\n \\mu_p^{R}(K,f_\\alpha) &\\triangleq \\mathrm{rank}\\big(\\,\\hat{\\partial}_{q}^{j + \\delta, k}\\,\\big) - \\; \\dots \\; + \\mathrm{rank}\\big(\\, \\hat{\\partial}_{q}^{i + \\delta, l}\\,\\big)  \\\\\n&\\equiv \\mathrm{rank}\\big(\\,V_p^j \\circ \\partial_{q} \\circ W_q^k \\,\\big) - \\; \\dots \\; + \\mathrm{rank}\\big(\\,V_p^{i+\\delta} \\circ \\partial_{q} \\circ W_q^l \\,\\big)\n \\end{align*}\n $$\n\nwhere the entries of $V$, $W$ change continuously w/ $\\alpha$, while $\\partial_q$ remains _fixed_...\n\n## Spectral functions {visibility=\"hidden\"}\n\nNuclear norm $\\lVert X \\rVert_\\ast = \\lVert \\mathbf{\\sigma} \\rVert_1$ often used in sparse minimization problems like _compressive sensing_ due to its convexity in the unit-ball $\\{A \\in \\mathbb{R}^{n \\times m} : \\lVert A \\rVert_2 \\leq 1 \\}$\n\n:::{layout=\"[[50,50]]\" layout-valign=\"bottom\"}\n\n![](images/l0_l1.png){width=300 height=100% fig-align=\"right\"}\n\n![](images/convex_envelope.png){width=320 height=100% fig-align=\"left\"}\n\n:::\n\n<div style=\"text-align: center;\"> \n\n__Left:__ The $\\ell_0$ and $\\ell_1$ norms on the interval $[-1,1]$\n\n__Right:__ $g$ forms the convex envelope of $f$ in the interval $[a,b]$\n\n</div> \n\n## Spectral functions {visibility=\"hidden\"}\n\nUnfortunately, $\\lVert \\cdot \\rVert_\\ast$ often a poor substitute for rank\n\n![](images/rank_relax.png){width=70% height=100% fig-align=\"center\"}\n\n__Left:__ The $\\ell_0$ and $\\ell_1$ norms on the interval $[-1,1]$\n__Right:__ \n\n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\n__Dataset:__ 3D meshes of animals in different poses [@chazal2009gromov]\n\n</div>\n\n![](images/gh_data_pose.png){width=425 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\n__Challenge:__ Recognize intrinsic shape categories (via a distance metric)\n\n</div>\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\nThe _Gromov-Hausdorff_ distance yields a metric on the set of compact metric spaces $\\mathcal{X}$\n\n$$\nd_{GH}(d_X, d_Y) = \\sup_{x \\in X, y \\in Y} \\lvert d_X(x, \\psi(y)) - d_Y(y, \\phi(x))\\rvert \n$$\n\n![](images/camel_gh.png){width=600 height=100% fig-align=\"center\"}\n\nUsing intrinsic metric makes $d_{\\mathrm{GH}}$ blind to e.g. shapes represented in different _poses_\n\n:::{.fragment}\n\n<div style=\"text-align: center;\"> \n\nUnfortunately, the GH distance is NP-hard to compute [@memoli2012some]\n\n</div>\n\n:::\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\nIt's known $d_B$ ($d_W$) on Rips filtrations $\\mathcal{R}(X, d_X)$ lower bound GH (GW) distance\n\n</div> \n\n$$ \nd_B(\\mathrm{dgm}_p(\\mathcal{R}(X, d_X)), \\mathrm{dgm}_p(\\mathcal{R}(Y, d_Y))) \\; \\leq \\; d_{GH}((X, d_X), (Y, d_Y))\n$$\n\n:::{layout-ncol=1}\n\n![](images/camel_gh_rips_comparison.png){width=875 height=100% fig-align=\"center\"}\n\n:::\n\n<div style=\"text-align: center;\"> \n\nMotivates use of persistence in metric settings for e.g. shape classification!\n\n</div> \n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\n__Issue:__ Diagrams are far from injective, cannot distinguish e.g. stretched shapes\n\n</div> \n\n![](images/dgm_noninjective.png){width=575 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\nThe lower bound on $d_{\\mathrm{GH}}$ could be totally useless!\n\n</div> \n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n:::{.fragment}\nLower bounds extend to Rips filtrations _augmented_ with real-valued functions $f, g$: \n\n$$\n\\mathcal{R}(f) \\triangleq \\mathcal{R}(X, d_X, f) = \\{\\mathcal{R}_\\alpha(X_\\alpha)\\}_{\\alpha > 0}, \\quad X_\\alpha \\triangleq f^{-1}((-\\infty, \\alpha)) \\subseteq X\n$$\n\n:::\n\n:::{.fragment}\n\nThe diagrams from $\\mathcal{R}(\\lambda \\cdot f_X)$ represent _stable signatures_ for each $\\lambda > 0$:\n\n$$\nd_B(\\mathcal{R}(\\lambda \\cdot f_X), \\mathcal{R}(\\lambda \\cdot f_Y)) \\leq \\max(1, \\lambda L) \\cdot d_{\\mathrm{GH}}(X, Y)\n$$\n\n:::\n\n<!-- <div style=\"text-align: center;\">  -->\n\n<!-- </div>  -->\n\n<!-- <hr>  -->\n\n:::{.fragment}\n\nChazal showed these bounds extend to metrics on _augmented_ metric spaces:\n\n$$\n\\mathcal{X}_1 = \\{ (X, d_X, f_X) \\mid (X, d_X, f_X) \\in \\mathcal{X}, f_X: X \\to \\mathbb{R} \\text{ continuous }\\}\n$$\n\nThese signatures also extend to measure metric spaces, see [@chazal2009gromov]\n\n::: \n\n:::{.fragment}\n\n<br>  \n\n<span style=\"color: red;\"> NOTE:   </span> \nSize of $L$ depends on the choice of $f$ + each $\\lambda$ produces a new signature! \n\n:::\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\n__Ex:__ The _eccentricity_ function $e_X^1(x) = \\max_{x' \\in X} d_X(x,x')$ has $L = 2$ \n\n</div> \n\n![](images/dgm_noninjective2.png){width=775 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\nAugmenting via a fraction of $e_X^1$ modifies the diagrams of the ellipsoid significantly\n<!-- , while the ones for the sphere hardly change due to the fact that the eccentricity is constant -->\n\n</div> \n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\nLower bounds extend to Rips filtrations _augmented_ with real-valued functions $f, g$: \n\n$$\nd_B(\\mathcal{R}(\\lambda \\cdot f_X), \\mathcal{R}(\\lambda \\cdot f_Y)) \\leq \\max(1, \\lambda L) \\cdot d_{\\mathrm{GH}}(X, Y)\n$$\n\n:::{layout-ncol=4}\n\n![](images/camel1_rips.png){width=450 height=100% fig-align=\"left\"}\n\n![](images/camel_dgm_rips.png){width=450 height=100% fig-align=\"left\"}\n\n![](images/camel1_ecc.png){width=450 height=100% fig-align=\"left\"}\n\n![](images/camel_dgm_ecc.png){width=450 height=100% fig-align=\"left\"}\n\n:::\n\n<div style=\"text-align: center;\"> \n\nLarger values of $\\lambda$ yield worse bounds, but can lead to simpler diagrams\n\n</div>\n\n<!-- Extra structure combines stability of persistence with flexibility of metrics -->\n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n\n<div style=\"text-align: center;\"> \n\nEach choice of $\\lambda > 0$ yields a _stable signature_ via $\\mathcal{R}(\\lambda \\cdot f_X)$\n\n</div> \n\n<div style=\"text-align: center;\"> \n\nWhich value of $\\lambda$ to choose?\n\n</div>\n\n![](images/camel1_interp1.png){width=950 height=100% fig-align=\"center\"}\n\n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\nEach choice of $\\lambda > 0$ yields a _stable signature_ via $\\mathcal{R}(\\lambda \\cdot f_X)$\n\n</div> \n\n<div style=\"text-align: center;\"> \n\nWhich value of $\\lambda$ to choose?\n\n</div>\n\n![](images/camel1_interp2.png){width=950 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\nWe sample from $\\Delta_+$ randomly, retaining signatures with sufficient topological activity\n\n</div>\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n...and compared the computed spectral signatures under the relative distance metric: \n\n<!-- \\partial_p^\\ast = U \\Sigma V^T -->\n$$\n\\Lambda(\\mu_p^R) = \\{\\sigma_1, \\sigma_2, \\dots, \\sigma_n \\}, \\quad \\quad \\chi(\\mathbf{\\sigma}, \\mathbf{\\tilde{\\sigma}}) = \\sum\\limits_{i=1}^n \\frac{\\lvert \\sigma_i - \\tilde{\\sigma}_i \\rvert}{\\sqrt{\\sigma_i + \\tilde{\\sigma}_i}}\n$$\n\n![](images/dw_chi_comp.png){width=750 height=100% fig-align=\"center\"}\n\n\n<!-- ## Experiment \\#3: Filtration optimization -->\n\n<!-- ![](images/smoothed_mu.png){width=55% height=100%} -->\n\n\n\n\n\n<!-- __Summary:__ We can obtain $\\mu_p^R(K, f_\\alpha)$ for varying $\\alpha$ by using thresholded versions of $f_\\alpha$ as scalar-products  -->\n\n## Overview {visibility=\"hidden\"}","srcMarkdownNoYaml":"\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n::: {.fragment .fade-in fragment-index=1 style=\"text-align: left\"}\n\n- Persistence Landscapes [@bubenik2020persistence]\n\n:::\n\n\n::: {.fragment .fade-in-then-out fragment-index=1 style=\"text-align: center\"}\n\n![](images/pers_landscape_def.png){width=40% fig-align=\"center\"}\n\n$$ \\lambda(k, t) = \\sup \\{ h \\geq 0 \\mid \\mathrm{rank}(H_p^{i-h} \\to H_p^{i+h}) \\geq k \\} $$\n\n:::\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n- Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay]\n\n![](images/pers_landscape_app.png){width=40% fig-align=\"center\"}\n\n$$ \\lambda(k, t) = \\sup \\{ h \\geq 0 \\mid \\mathrm{rank}(H_p^{i-h} \\to H_p^{i+h}) \\geq k \\} $$\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n<ul> \n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>\n\n:::\n\n::: {.fragment .fade-in fragment-index=1 style=\"text-align: left\"}\n\n<li> Persistence Images [@adams2017persistence] </li> \n\n:::\n\n<ul> \n\n\n::: {.fragment .fade-in-then-out fragment-index=1 style=\"text-align: center\"}\n\n![](images/pers_image_def.png){height=50% fig-align=\"center\"}\n\n$$ \\rho(f, \\phi) = \\sum\\limits_{(i,j) \\in \\mathrm{dgm}} f(i,j) \\phi(\\lvert j - i \\rvert)$$\n\n:::\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n<ul> \n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>\n\n:::\n\n<li> Persistence Images [@adams2017persistence] + Learning applications [@som2020pi]\n</li> \n\n<ul> \n\n![](images/pers_image_app.png){height=50% fig-align=\"center\"}\n\n$$ \\rho(f, \\phi) = \\sum\\limits_{(i,j) \\in \\mathrm{dgm}} f(i,j) \\phi(\\lvert j - i \\rvert)$$\n\n\n\n## Learning with persistence \n\nThere are many mappings from $\\mathrm{dgm}$'s to function spaces (e.g. Hilbert spaces) \n\n<ul> \n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Landscapes [@bubenik2020persistence] + Learning applications [@kim2020pllay] </li>\n\n:::\n\n::: { style=\"color: rgb(127,127,127);\"}\n\n<li> Persistence Images [@adams2017persistence] + Learning applications [@som2020pi] </li>\n\n:::\n\n::: {.fragment .fade-in}\n\n<li> A few others...$^1$ </li> \n\n![](images/vec1.png){width=80% height=100% fig-align=\"center\"}\n\n:::\n\n</ul>\n\n:::{.aside}\n\nSee [@bubenik2020persistence] for an overview. \n\n:::\n\n## Many goals in common...\n\n<hr/> \n\n:::: {.columns}\n\n::: {.column width=40% layout-align=\"left\" style=\"margin-left: 2em; margin-top: 1em;\"}\n\n::: {.fragment fragment-index=1}\n\n- Vectorize persistence information\n\n:::\n\n::: {.fragment fragment-index=2}\n\n- Optimize persistence invariants \n\n:::\n\n::: {.fragment fragment-index=3}\n\n- Leverage the stability of persistence\n\n:::\n\n\n::: {.fragment fragment-index=4}\n\n- Connect to other areas of mathematics\n\n:::\n \n:::\n\n::: {.column width=40% layout-align=\"left\"}\n\n:::{.r-stack}\n\n![](images/pers_image.png){.fragment fragment-index=1 width=\"300\" height=\"300\"}\n\n![](images/pers_landscape_app.png){.fragment fragment-index=2 width=\"300\" height=\"300\"}\n\n![](animations/stability.gif){.fragment fragment-index=3 width=\"400\" height=\"300\"}\n\n![](images/lsst.png){.fragment fragment-index=4 width=\"375\" height=\"375\"}\n\n:::\n\n:::\n\n:::: \n\n::: {.fragment style=\"text-align: center\" style=\"font-size: 40px;\"}\n\n<div style=\"text-align: center; font-size: 35px;\" >\n\n__Can we achieve these goals without computing diagrams?__\n\n</div>\n\n\n:::\n\n::: {.fragment style=\"text-align: center\" style=\"font-size: 40px;\"}\n\n(computing $\\mathrm{dgm}_p(X)$ on point cloud inputs $X \\subset \\mathbb{R}^{n \\times d}$ takes $O(n^{3(p+2)})$ for $p \\geq 1$...)\n:::\n\n<!-- ::: {.fragment style=\"text-align: center\"}\n\n$^\\ast$ _avoid the reduction the algorithm_\n\n:::\n\n::: {.fragment style=\"text-align: center\"}\n\n$$\\mathrm{dgm}(K) \\leftrightarrow R = \\partial V $$\n\n::: -->\n\n<!-- :::{.aside}\n\nImage from: https://epfl-lts2.github.io/gspbox-html/doc/graphs/\n\n::: -->\n\n<br> \n\n## This Talk - Spectral Rank Invariants {visibility=\"hidden\"}\n\n:::{.fragment .fade-in style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates the _persistent rank_ function\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n3. \"Matrix-free\" computation in $O(n)$ memory \n4. Variety of applications, e.g. featurization, optimization, metric learning\n\n</div>\n\n:::\n\n:::{.fragment .fade-in style=\"text-align: left\" layout=\"[[48,52]]\" layout-valign=\"top\"}\n\n![](animations/ph_transform.gif){width=100% height=100% fig-align=\"left\"}\n\n![](animations/trace_summary.gif){width=100% height=100% fig-align=\"left\"}\n\n:::\n\n:::{.aside}\n\\(1\\) The Schatten-1 norms of the operators driving the relaxation are differentiable over the positive semi-definite cone \n:::\n\n## This Talk - Spectral Rank Invariants \n\n:::{.fragment .fade-in style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n:::\n\n\n:::{.fragment .fade-in style=\"text-align: center\"}\n\n<div style=\"margin-left:2.5em;\">\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n</div>\n\n<br/> \n\n:::{style=\"text-align: center\"}\n\n![](images/spectral_interp2.png){height=450 fig-align=\"center\"}\n\n:::\n\n:::\n\n<!-- :::{layout=\"[[80, 50]]\" layout-valign=\"bottom\"}\n\n![](images/size_function.png){height=50% fig-align=\"center\"}\n![](images/spectral_interpolation.png){height=50% fig-align=\"center\"}\n\n:::\n::: -->\n\n\n## This Talk - Spectral Rank Invariants \n\n:::{style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n\n</div>\n\n<div style=\"width: 100%; text-align: center;\">\n![](images/stochastic_trace_estimates.png){width=80% fig-align=\"center\"}\n</div>\n\n:::\n\n## This Talk - Spectral Rank Invariants \n\n:::{style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n3. \"Matrix-free\" computation in $\\approx O(n)$ memory \n\n</div>\n\n<br/> \n\n$$ {\\Large v^T (\\partial_1 \\circ \\partial_1^T) v = \\sum\\limits_{i \\sim j}(v_i - v_j) }$$\n\n:::\n\n## This Talk - Spectral Rank Invariants \n\n:::{style=\"text-align: center\"}\n\nWe introduce a _spectral-relaxation_ of the <span style=\"color:orange\">  persistent rank invariants </span>  $\\beta_p^{\\ast}$ and $\\mu_p^\\ast$ that:\n\n<div style=\"margin-left:2.5em;\">\n\n1. Smoothly interpolates _persistent rank function_ $\\leftrightarrow$ _spectral sum_\n2. Admits $(1 \\pm \\epsilon)$ approximation for any $\\epsilon > 0$ in $\\approx O(n^2)$ time \n3. \"Matrix-free\" computation in $\\approx O(n)$ memory \n4. Variety of applications, e.g. featurization, optimization, metric learning\n\n</div>\n\n![](images/overview_blackbox.png){width=90% fig-align=\"center\"}\n\n:::\n\n## Application: optimizing filtrations {visibility=\"hidden\"}\n\n![](images/combined_mult.png){width=68% height=100% fig-align=\"center\"}\n\n$$ \\alpha^\\ast = \\argmax_{\\alpha \\in \\mathbb{R}} \\; \\mathrm{card}\\big(\\, \\left.\\mathrm{dgm}(K_\\bullet, \\, f_\\alpha) \\right|_{R} \\, \\big) $$\n\n## Application: _sifting_ bifiltrations\n\n:::{.fragment}\n\nSuppose you want to infer whether data lies in the vicinity of some topological space\n\n:::\n\n:::{.fragment}\n\n::::{.columns}\n\n:::{.column}\n\n![](images/patch_manifold.png){fig-align=\"right\" style=\"max-width: 350px !important; max-height: 350px !important;\"}\n\n:::\n\n:::{.column}\n\n![](images/five_circle.png){fig-align=\"left\" style=\"max-width: 350px !important; max-height: 350px !important;\"}\n\n:::\n\n::::\n\n:::\n\n:::{.fragment}\n\n<div style=\"text-align: center;\">\n\nPersistence may fail to find evidence of this space due to _strong outliers_\n\n</div>\n\n:::\n\n:::{.fragment}\n\n$${\\large \\text{ Solution: add a (co)-density filter }}$$\n\n:::\n\n## Application: _sifting_ bifiltrations\n\n<div style=\"text-align: center;\">\n\nSuppose we filter the data by _diameter_ and _codensity_\n\n</div>\n\n<!-- ::::{.columns}\n\n:::{.column} -->\n\n![](images/hilbert_unmarked.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n<!-- :::\n\n\n:::{.column}\n\n:::\n\n\n:::: -->\n\n<!-- <div style=\"text-align: center;\">\n\nMulti-parameter persistence can remedy this situation!\n\n</div> -->\n\n\n## Application: _sifting_ bifiltrations\n\n<div style=\"text-align: center;\">\n\nSuppose we filter the data by _diameter_ and _codensity_\n\n</div>\n\n<!-- ::::{.columns}\n\n:::{.column} -->\n\n![](images/hilbert_marked.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n<!-- :::\n\n\n:::{.column}\n\n:::\n\n\n:::: -->\n\n<div style=\"text-align: center;\">\n\n...and we highlight the constant value regions with dimension $5$\n\n</div>\n\n\n\n## Application: _sifting_ bifiltrations\n\n<div style=\"text-align: center;\">\n\nSuppose we restrict the bifiltration to line & compute its 1d persistence\n\n</div>\n\n::::{.columns}\n\n:::{.column} \n\n![](images/hilbert_marked_dim.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n:::\n\n\n:::{.column}\n\n![](images/pers5.png){fig-align=\"center\" style=\"max-width: 450px !important; max-height: 450px !important;\"}\n\n:::\n\n\n::::\n\n:::{.fragment}\n\n<div style=\"text-align: center;\">\n\nMulti-parameter persistence can remedy this situation!\n\n</div>\n\n:::\n\n## Application: _sifting_ bifiltrations\n\n\n![](images/hilbert_scanning2.png){fig-align=\"center\" style=\"max-width: 1150px !important; max-height: 1150px !important;\"}\n\n## Why the rank invariant {visibility=\"hidden\"}\n\n\n:::{.fragment}\n\nThere is a duality between diagrams its associated rank function:\n\n$$ \\mathrm{dgm}_p(\\, K_\\bullet, \\, f \\, ) \\triangleq \\{ \\, ( \\, i, j \\,) \\in \\Delta_+ :  \\mu_p^{i,j} \\neq 0 \\, \\} \\; \\cup \\; \\Delta $$\n\n$$\\text{where: } \\quad \\mu_p^{i,j} = \\left(\\beta_p^{i,j{\\small -}1} - \\beta_p^{i,j} \\right) - \\left(\\beta_p^{i{\\small -}1,j{\\small -}1} - \\beta_p^{i{\\small -}1,j} \\right) \\quad $$\n\n:::\n\n:::{.fragment}\n\n_Fundamental Lemma of Persistent Homology_ shows diagrams characterize their ranks\n$$\\beta_p^{k,l} = \\sum\\limits_{i \\leq k} \\sum\\limits_{j > l} \\mu_p^{i,j}$$\n\n:::\n\n:::{.incremental}\n\n- _Persistence measures_ [@chazal2016structure] extend (1,2) naturally when $\\mathbb{F} = \\mathbb{R}$ \n- Stability in context of multidimensional persistence [@cerri2013betti] \n- Generalizations of rank invariant via Möbius inversion [@mccleary2022edit] and via zigzag persistence[@dey2021computing]\n\n:::\n\n## Overview\n\n<ul>\n  <li>Theory</li>\n  <ul>\n    <li>Rank duality</li>\n    <li>Spectral functions</li>\n    <li>Operator properties</li>\n  </ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Applications</p></li>\n  <ul>\n    <li>Variational + geometric perspective</li>\n    <li>Filtration optimization</li>\n    <li>Feature generation</li>\n  </ul>\n  </div>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Computation (time permitting)</p></li>\n  <ul>\n    <li>Implicit trace estimation</li>\n    <li>Stochastic Lanczos quadrature</li>\n    <li>matvecs are all you need</li>\n  </ul>\n  </div>\n</ul>\n\n\n## The rank invariants {style=\"text-align: center;\"}\n\n::::{.columns}\n\n:::{.column}\n\n![](images/dgm_pbn.png){width=325 height=325 fig-align=\"center\"}\n\n<div style=\"text-align: center;\">\n\n$\\beta_p^{i,j}(K)$\n\n</div>\n\n:::\n\n:::{.column}\n\n![](images/dgm_mu.png){width=325 height=325 fig-align=\"center\"}\n\n<div style=\"text-align: center;\">\n\n$\\mu_p^R(K)$\n\n</div>\n\n:::\n\n::::\n\n## \n\n\n![](images/function_persistence.png){width=875 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::{.fragment}\n$$ \\mathrm{dgm}_p(K, f) \\triangleq \\{ \\, (a, b) :  \\mu_p^{a,b} \\neq 0  \\, \\} $$\n\n$$ \\mu_p^R(K, f) = \\beta_{p}^{b,c} - \\beta_{p}^{a,c} - \\beta_{p}^{b,d} + \\beta_{p}^{a,d}, $$\n\n:::\n\n\n<!-- $$\\mu_p^R(K, f) \\triangleq \\mathrm{card}\\left(\\mathrm{dgm}_p(K, f) \\mid_R \\right) = \\beta_{p}^{b,c} - \\beta_{p}^{a,c} - \\beta_{p}^{b,d} + \\beta_{p}^{a,d}$$  -->\n\n\n##\n\n![](images/size_function2.png){width=875 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n$$\n\\begin{align*}\n\\beta_p : \\; & \\;\\; \\Delta_+ & \\to & \\quad \\mathbb{Z}_+ &&  \\\\\n& (a,b) & \\mapsto & \\quad \\mathrm{rank}(H_p(K_a) \\to H_p(K_b)) && \n\\end{align*}\n$$\n\n\n## The rank invariant {style=\"text-align: center;\"}\n\n__Goal:__ \"Relax\" this integer-valued function via _spectral_ characterization of rank:\n\n![](images/spectral_interpolation_3.png){width=975 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::{.fragment .fade-in-then-semi-out}\n\n$$ \n\\begin{equation}\n\\mu_p^{a, b} \\triangleq \\; \\min_{\\delta > 0} \\left(\\beta_p^{a + \\delta, b  - \\delta} - \\beta_p^{a + \\delta, b  + \\delta} \\right) - \\left(\\beta_p^{a- \\delta, b - \\delta} - \\beta_p^{a - \\delta, b + \\delta} \\right) \n\\end{equation} \n$$\n\n:::\n\n\n:::{.fragment}\n$$ \n\\begin{equation}\n\\beta_p^{a,b} = \\mathrm{rank}(H_p(K_a) \\to H_p(K_b)) \\quad \\Leftrightarrow \\quad \\mathrm{rank}(X) = \\sum_{i=1}^n \\, \\mathrm{sgn}_+(\\sigma_i)\n\\end{equation} \n$$\n:::\n\n<!-- \n## Restrictions & Implications\n\n\n:::{style=\"text-size: 14px; text-align: center; margin-top: 1em;\"}\n\n\n1. Field coefficients in $\\mathbb{R}$<sup>1</sup>\n2. Filtrations ordered simplexwise _reverse colexicographically_\n\n:::\n\n<br/> \n\n$$\n\t{\\color{green} \\mu_p^{R}} = \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{j + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{i + 1, l} }\n\t\\end{bmatrix}\n\t- \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{i + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{j + 1, l} }\n\t\\end{bmatrix}\n$$\n\n<br/>\n\n:::{style=\"text-size: 14px; text-align: center;\"}\n\n:::{.fragment}\n\nThere are advantages to preferring _this_ expression for $\\mu_p^R$\n\n:::\n\n<ol>\n:::{.fragment}\n  <li> <span style=\"color: blue;\"> Inner terms </span> are _unfactored_ </li>\n:::\n:::{.fragment}\n  <li> Variational perspectives on <span style=\"color: red;\">rank function </span> well-studied ($\\mathbb{R}$)</li>\n:::\n:::{.fragment}\n  <li> Theory of <span style=\"color: green;\">_persistent measures_$^{\\ast}$</span> readily applicable </li>\n:::\n</ol>\n\n:::\n\n::: aside \n\n1. Or with a zero-characteristic field.\n::: -->\n\n\n## Key technical observation {visibility=\"hidden\"}\n\n:::{.fragment style=\"text-align: center;\"}\n\nPairing uniqueness lemma can be used to show: \n\n$$ \n\\text{ if } (i,j) \\in \\mathrm{dgm}(K_\\bullet), \\text{ and } R = \\partial V\n$$\n \n:::\n\n:::{.fragment style=\"margin: 0; padding: 0;\"}\n\n$$\n\\implies \\mathrm{rank}(R^{i,j}) - \\mathrm{rank}(R^{i\\texttt{+}1,j}) + \\mathrm{rank}(R^{i\\texttt{+}1,j\\text{-}1}) - \\mathrm{rank}(R^{i,j\\text{-}1}) \\neq 0\n$$\n\n![](images/rank_ll.png){width=575 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::\n\n:::{.fragment}\n\n$$\n\\Rightarrow \\mathrm{rank}(R^{i,j}) = \\mathrm{rank}(\\partial^{i,j}) \n$$\n\n:::\n\n## Key technical observation\n\n:::{.fragment }\n\n![](images/rv_ll.png){width=950 height=100% fig-align=\"center\" style=\"margin: 0; padding: 0;\"}\n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\n__Pairing uniqueness lemma__<sup>1</sup>  $\\implies \\; \\mathrm{rank}(R^{i,j}) = \\mathrm{rank}(\\partial^{i,j})$\n\n:::\n\n<br/> \n\n:::{.fragment style=\"text-align: center; font-size: 38px;\"}\n\n$\\Leftrightarrow$ Can deduce $\\mathrm{dgm}$'s from ranks of \"lower-left\" blocks of $\\partial_p(K_\\bullet)$\n\n:::\n\n\n:::aside\n\n1. Cohen-Steiner, David, Herbert Edelsbrunner, and Dmitriy Morozov. \"Vines and vineyards by updating persistence in linear time.\" Proceedings of the twenty-second annual symposium on Computational geometry. 2006.\n\n:::\n\n## Key technical observation\n\n$$ \n\\begin{equation}\n\\mathrm{rank}(R^{i,j}) = \\mathrm{rank}(\\partial^{i,j})  \n\\end{equation}\n$$\n \n<hr>\n\n<!-- :::{.fragment style=\"text-align: center;\"}\n\n$(1)$ often used to show correctness of reduction, but far more general:\n\n::: -->\n\n:::{.fragment}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 2em; \">\n__Corollary [@bauer2022keeping]__: &nbsp;Any algorithm that preserves the ranks of the submatrices $\\partial^{i,j}$ for all $i,j \\in \\{ 1, \\dots, n \\}$ is a valid barcode algorithm.\n</div>\n\n:::\n\n:::{.fragment}\n$$ \n\\begin{equation}\n(1) \\Rightarrow \\beta_p^{i,j} = \\lvert C_p(K_i) \\rvert - \\mathrm{rank}(\\partial_p^{1,i}) - \\mathrm{rank}(\\partial_{p+1 }^{1,j}) + \\mathrm{rank}(\\partial_{p+1}^{i + 1, j} ) \n\\end{equation}\n$$\n\n:::\n\n\n:::{.fragment}\n\n$$ \n\\begin{equation}\n(2) \\Rightarrow \\mu_p^{R} = \\mathrm{rank}(\\partial_{p+1}^{j + 1, k})  - \\mathrm{rank}(\\partial_{p+1}^{i + 1, k})  - \\mathrm{rank}(\\partial_{p+1}^{j + 1, l}) + \\mathrm{rank}(\\partial_{p+1}^{i + 1, l})  \n\\end{equation}\n$$\n\n:::\n\n:::{.aside}\n\n@edelsbrunner2000topological noted (1) in passing showing correctness of reduction; @dey2022computational explicitly prove (2); (3) was used by @chen2011output. (2) & (3) are connected to relative homology.\n\n:::\n\n\n## Restrictions & Implications\n\n\n:::{style=\"text-size: 14px; text-align: center; margin-top: 1em;\"}\n\nWe restrict<sup>1</sup> to persistence with field coefficients in $\\mathbb{R}$\n\n:::\n\n<br/> \n\n$$\n\t{\\color{green} \\mu_p^{R}} = \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{j + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{i + 1, l} }\n\t\\end{bmatrix}\n\t- \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\partial_{p+1}^{i + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\partial_{p+1}^{j + 1, l} }\n\t\\end{bmatrix}\n$$\n\n<br/>\n\n:::{style=\"text-size: 14px; text-align: center;\"}\n\n:::{.fragment}\n\nThere are advantages to preferring _this_ expression for $\\mu_p^R$\n\n:::\n\n<ol>\n:::{.fragment}\n  <li> <span style=\"color: blue;\"> Inner terms </span> are _unfactored_ </li>\n:::\n:::{.fragment}\n  <li> Variational perspectives on <span style=\"color: red;\">rank function </span> well-studied ($\\mathbb{R}$)</li>\n:::\n:::{.fragment}\n  <li> Theory of <span style=\"color: green;\">_persistent measures_<sup>2</sup></span> readily applicable </li>\n:::\n</ol>\n\n:::\n\n::: aside \n\n1. Or any zero characteristic field\n2. Chazal, Frédéric, Vin De Silva, Marc Glisse, and Steve Oudot. 2016. The Structure and Stability of Persistence Modules.\n\n:::\n\n\n\n## Restrictions & Implications\n\n\n:::{style=\"text-size: 14px; text-align: center; margin-top: 1em;\"}\n\nWe restrict<sup>1</sup> to persistence with field coefficients in $\\mathbb{R}$\n\n:::\n\n<br/> \n\n$$\n\t{\\color{green} \\mu_p^{R}} = \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\mathcal{L}_{p}^{j + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\mathcal{L}_{p}^{i + 1, l} }\n\t\\end{bmatrix}\n\t- \n\t{\\color{red} \\mathrm{rank}}\\begin{bmatrix} {\\color{blue} \\mathcal{L}_{p}^{i + 1, k}} & 0 \\\\\n\t0 & {\\color{blue} \\mathcal{L}_{p}^{j + 1, l} }\n\t\\end{bmatrix}\n$$\n\n<br/>\n\n:::{style=\"text-size: 14px; text-align: center;\"}\n\nThere are advantages to preferring _this_ expression for $\\mu_p^R$\n\n<ol>\n  <li> <span style=\"color: blue;\"> Inner terms </span> are _unfactored_ </li>\n  <li> Variational perspectives on <span style=\"color: red;\">rank function </span> well-studied ($\\mathbb{R}$)</li>\n  <li> Theory of <span style=\"color: green;\">_persistent measures_<sup>2</sup></span> readily applicable </li>\n</ol>\n\n:::\n\n::: aside \n\n1. Or any zero characteristic field\n2. Chazal, Frédéric, Vin De Silva, Marc Glisse, and Steve Oudot. 2016. The Structure and Stability of Persistence Modules.\n\n:::\n\n## Relaxing the rank function {visibility=\"hidden\"}\n\n:::{style=\"text-align: center\"} \n\n__Spectral relaxation__: Approximate $\\mathrm{rank}$ with _matrix functions_ [@bhatia2013matrix]\n\n:::\n\n\n:::{style=\"list-style-type: none; align=center;\"}\n\n<div class=\"columns\">\n\n<div class=\"column\">\n\n::: {.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: left\"}\n\n$\\quad\\quad\\quad\\quad \\mathrm{rank}(X) = \\sum \\, \\mathrm{sgn}_+(\\sigma_i)$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=2 style=\"text-align: left\"}\n\n$\\quad\\quad\\quad\\quad \\hphantom{\\mathrm{rank}(X)}\\approx \\sum\\limits_{i=1}^n \\, \\phi(\\sigma_i, \\tau) \\phantom{\\int\\limits_{-\\infty}^x}$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=3 style=\"text-align: left\"}\n\n$\\quad\\quad\\quad\\quad \\hphantom{\\mathrm{rank}(X)}=\\lVert \\Phi_\\tau(X) \\rVert_\\ast$\n\n:::\n\n</div>\n\n<div class=\"column\">\n\n::: {.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: right\"}\n\nwhere $\\quad\\quad$ $X = U \\mathrm{Diag}(\\mathbf{\\sigma})V^T$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=2 style=\"text-align: right\"}\n\nwhere $\\quad \\phi(x, \\tau) \\triangleq \\int\\limits_{-\\infty}^x\\hat{\\delta}(z, \\tau) dz$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=3 style=\"text-align: right\"}\n\nwhere $\\quad \\Phi_\\tau(X) \\triangleq \\sum_{i=1}^n \\phi(\\sigma_i, \\tau) u_i v_i^T$\n\n:::\n\n</div>\n\n</div>\n\n::: {.fragment .fade-in-then-semi-outstyle=\"text-align: center\"}\n\n$\\Phi_\\tau(X)$ is a _Löwner operator_ when $\\phi$ is _operator monotone_ [@jiang2018unified]\n\n$$ A \\succeq B \\implies \\Phi_\\tau(A) \\succeq \\Phi_\\tau(B) $$\n\n:::\n\n:::{.fragment style=\"text-align: center\"}\n\nClosed-form proximal operators exist when $\\Phi_\\tau$ convex [@beck2017first]\n\nOften used in nonexpansive mappings [@bauschke2011convex]\n\n:::\n\n:::\n\n## Relaxing the rank function\n\n<hr/> \n\n:::{.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: center\"} \n\n__Spectral relaxation__: Approximate $\\mathrm{rank}$ by approximating the _sign_ function \n\n:::\n\n\n:::{style=\"list-style-type: none; align=center;\"}\n\n::: {.fragment .fade-in-then-semi-out fragment-index=1 style=\"text-align: center\"}\n\n$$\\mathrm{rank}(X) = \\mathrm{rank}(X X^T) = \\sum_{i=1}^n \\, \\mathrm{sgn}_+(\\lambda_i) \\approx \\sum_{i=1}^n \\, \\phi(\\lambda_i, \\tau) $$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out fragment-index=2 style=\"text-align: left\"}\n\n$$\\text{ where } \\phi(x, \\tau) \\triangleq \\int\\limits_{-\\infty}^x\\hat{\\delta}(z, \\tau) dz \\text{ for a smoothed Dirac measure } \\hat{\\delta}^1$$\n\n<!-- \\hat{\\delta}(x, \\tau) = \\frac{1}{\\nu(\\tau)} p\\left(\\frac{x}{\\nu(\\tau)}\\right), \\quad \\tau > 0, \\quad \\nu \\text{ inc. } -->\n:::\n\n\n::: {.fragment .fade-in-then-semi-out fragment-index=3 style=\"text-align: center\"}\n\n$\\phi : \\mathbb{R} \\to \\mathbb{R}$ induces a unique$^2$ _spectral function_ $F: S_{n} \\to \\mathbb{R}$ via its _trace_: \n\n$$\\mathrm{tr}(\\Phi_\\tau(X)) = \\sum\\limits_{i=1}^n \\phi(\\lambda_i, \\tau), \\quad \\Phi_\\tau(X) \\triangleq U \\phi_\\tau(\\Lambda) U^T $$\n\n:::\n\n\n<!-- ::: {.fragment .fade-in-then-semi-out fragment-index=4 style=\"text-align: center\"}\n\nThe operator $\\Phi_\\tau(X) \\in \\mathbb{R}^{n \\times n}$ is called a _Löwner operator_ [@bhatia2013matrix]\n\n::: -->\n\n\n<!-- ::: {.fragment .fade-in-then-semi-out fragment-index=4 style=\"text-align: center\"}\n\n$\\Phi_\\tau(X)$ is a _Löwner operator_ when $\\phi$ is _operator monotone_ [@jiang2018unified]\n\n$$ A \\succeq B \\implies \\phi_\\tau(A) \\succeq \\phi_\\tau(B) $$\n\n:::\n\n:::{.fragment style=\"text-align: center\"}\n\nClosed-form proximal operators exist when $\\phi_\\tau$ convex + minor conditions$^1$ \n\n::: -->\n\n:::\n\n<aside class=\"aside\" style=\"text-align: center !important;\">\n\n\\(1\\) Any $\\hat{\\delta}$ of the form $\\nu(1/\\tau) p (z \\cdot \\nu (1/\\tau))$ where $p$ is a density function and $\\nu$ positive and increasing is sufficient.\n\n\\(2\\) See Theorem 1.2 of @jiang2018unified for uniqueness conditions.\n<!-- \\(1\\) See @beck2017first and @bauschke2011convex for existence and optimality conditions.  -->\n\n</aside>\n\n\n## Spectral functions\n\nFor any smoothed Dirac measure^[Any $\\hat{\\delta}$ of the form $\\nu(1/\\tau) p (z \\cdot \\nu (1/\\tau))$ where $p$ is a density function and $\\nu$ positive and increasing is sufficient.] $\\hat{\\delta}$ and _operator monotone_ $\\phi: \\mathbb{R}_+ \\times \\mathbb{R}_{++} \\to \\mathbb{R}_+$, @bi2013approximation show that:\n\n<!-- <div class=\"columns\" style=\"margin-left: 2.5em; \"> -->\n\n<div style=\"list-style-type: none !important;\">\n\n<div class=\"columns\">\n\n<div class=\"column\" style=\"width: 30%\">\n\n:::{.fragment .fade-in fragment-index=1 .no_bullet}\n\n($\\tau$-approximate) $\\vphantom{\\hat{\\delta}}$\n\n:::\n\n:::{.fragment .fade-in fragment-index=2}\n\n(Monotone) $\\vphantom{\\lVert \\phi_{\\tau}(X) \\rVert_\\ast}$\n\n:::\n\n:::{.fragment .fade-in fragment-index=3}\n\n(Smooth) $\\vphantom{\\mathbb{R}_1^{n \\times m^{\\ast^{\\ast}}}}$\n\n:::\n\n:::{.fragment .fade-in fragment-index=4}\n\n(Explicit) $\\vphantom{\\partial \\lVert \\Phi_\\tau(\\cdot) \\rVert_\\ast}$ \n\n:::\n\n</div>\n\n<div class=\"column\" width=\"70%\" layout-align=\"right\">\n\n:::{.fragment .fade-in fragment-index=1}\n\n$0 \\leq \\mathrm{rank}(X) - \\lVert \\Phi_\\tau(X) \\rVert_\\ast \\leq c(\\hat{\\delta})$\n\n:::\n\n:::{.fragment .fade-in fragment-index=2}\n\n$\\lVert \\Phi_{\\tau}(X) \\rVert_\\ast \\geq \\lVert \\Phi_{\\tau'}(X) \\rVert_\\ast$ for any $\\tau \\leq \\tau'$\n\n:::\n\n:::{.fragment .fade-in fragment-index=3}\n\nSemismooth^[Here _semismooth_ refers to the existence of directional derivatives] on $\\mathbb{R}^{n \\times m}$ $\\vphantom{\\mathbb{R}_1^{n \\times m^{\\ast^{\\ast}}}}$, differentiable on $\\mathbf{S}_+^m$\n\n:::\n\n:::{.fragment .fade-in fragment-index=4}\n\nDifferential $\\partial \\lVert \\Phi_\\tau(\\cdot) \\rVert_\\ast$ has closed-form soln.\n\n:::\n\n</div>\n\n</div>\n\n:::{.fragment .fade-in fragment-index=5}\n\nFunction/operator pairs ( $\\phi_\\tau$, $\\Phi_\\tau$ ) particular specializations of _matrix functions_:\n\n$$\\Phi_\\tau(X) = U \\phi_\\tau(\\Lambda) U^T$$\n\nCommonly used in many application areas, e.g. compressed sensing [@li2014new]\n\n:::\n\n</div>\n\n## Combinatorial Laplacian {visibility=\"hidden\"}\n\n__Relax #3:__ Replace $\\partial \\mapsto L$ with _combinatorial Laplacians_ [@horak2013spectra]:\n\n$$ \\Delta_p = \\underbrace{\\partial_{p+1} \\partial_{p+1}^T}_{L_p^{\\mathrm{up}}}  + \\underbrace{\\partial_{p}^T \\partial_{p}}_{L_p^{\\mathrm{dn}}} $$\n\n:::{.fragment}\n\n$f_\\alpha$ is 1-to-1 correspondence with inner products on cochain groups $C^p(K, \\mathbb{R})$ \n\n$$L_p^{i,j}(\\alpha) \\Leftrightarrow \\langle \\; f,\\, g \\; \\rangle_{\\alpha} \\; \\text{ on } \\;  C^{p+1}(K, \\mathbb{R})$$\n\n::: \n\n:::{.fragment}\n\nBenefits: Symmetric, positive semi-definite, have \"nice\" linear and quadratic forms:\n$$\nL_p^{\\text{up}}(\\tau, \\tau')= \\begin{cases}\n\t\t \\mathrm{deg}_f(\\tau) \\cdot f^{+/2}(\\tau) & \\text{ if } \\tau = \\tau' \\\\ \n%\t\t\\mathrm{deg}(\\tau_i) & \\text{ if } i = j \\\\ \n\t\ts_{\\tau, \\tau'} \\cdot  f^{+/2}(\\tau) \\cdot f(\\sigma) \\cdot f^{+/2}(\\tau') & \\text{ if } \\tau \\overset{\\sigma}{\\sim} \\tau' \\\\\n\t\t0 & \\text{ otherwise} \n\t\\end{cases}\n$$\n\n$\\implies$ can represent operator in \"matrix-free\" fashion\n:::\n\n## Parameterized filtrations {visibility=\"hidden\"}\n\nSuppose we have an $\\alpha$-parameterized filtration $(K, f_\\alpha)$ where $f_\\alpha : K \\to \\mathbb{R}_+$ satisfies:\n\n$$\nf_\\alpha(\\tau) \\leq f_\\alpha(\\sigma) \\quad \\text{ if } \\tau \\subseteq \\sigma \\quad \\forall \\tau,\\sigma \\in K \\text{ and } \\alpha \\in \\mathbb{R}\n$$\n\n:::{layout-ncol=2}\n\n![](animations/codensity_family.gif){width=48% height=100% fig-align=\"right\"}\n\n![](animations/complex_plain.gif){width=48% height=100% fig-align=\"left\"}\n\n:::\n\n## __Relax \\#1__: Parameterized _boundary matrices_ {visibility=\"hidden\"}\n\n:::{.fragment style=\"text-align: center;\"}\n\nParameterize $C_p(K; \\mathbb{R})$ with $\\mathcal{S} \\circ f_\\alpha : K \\to \\mathbb{R}_+$ where $\\mathcal{S}: \\mathbb{R} \\to [0,1]$  \n\n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\n![](images/smoothstep_3.jpeg){width=88% height=100% fig-align=\"center\"}\n\n:::\n\n\n:::{.fragment style=\"text-align: center; border: 1;\"}\n\n$$ \n\\boxed{\n\\partial_p^{i,j}(\\alpha) = D_p(\\mathcal{S}_i \\circ f_\\alpha) \\circ \\partial_p(K_\\preceq) \\circ D_{p+1}(\\mathcal{S}_j \\circ f_\\alpha) \n}\n$$ \n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\n__Note__: $P^T \\partial_p^{i,j}(\\alpha) P$ has rank $= \\mathrm{rank}(R_p^{i,j}(\\alpha))$ for all $\\alpha \\in \\mathbb{R}$. \n\n:::\n\n:::{.aside style=\"text-align: center;\"}\nReplacing $S \\mapsto \\mathcal{S}$ ensures continuity of $\\partial_p^{i,j}(\\alpha)$\n:::\n\n\n## Rank Invariances when $\\mathbb{F} = \\mathbb{R}$ {visibility=\"hidden\"}\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n&emsp;&emsp;&emsp;&emsp;\n\n$\\hspace{10em} \\mathrm{rank}(A) \\triangleq \\mathrm{dim}(\\mathrm{Im}(A))$\n\n::: \n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(A^T) \\quad \\quad  \\quad \\text{(adjoint)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(A^T A) \\quad \\quad \\; \\text{(inner product)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(A A^T) \\quad \\quad \\; \\text{(outer product)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(S^{-1}AS) \\quad \\;  \\text{(change of basis)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\mathrm{rank}(P^T A P) \\quad \\; \\text{(permutation)}$\n\n:::\n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n$\\hspace{10em}  \\hphantom{\\mathrm{rank}(A)} \\equiv \\dots  \\quad \\quad \\quad \\quad  \\quad \\quad  \\! \\! \\text{(many others)}$\n\n:::\n\n<br> \n\n::: {.fragment .fade-in style=\"text-align: left\"}\n\n<div style=\"text-align: center; font-size: 35px;\" >\n\n__Q: Can we exploit some of these to speed up the computation?__\n\n</div>\n\n:::\n\n## Overview\n\n<ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li>Theory</li>\n  <ul>\n    <li>Rank duality</li>\n    <li>Spectral functions</li>\n    <li>Operator properties</li>\n  </ul>\n  </div>\n  <li><p>Applications</p></li>\n  <ul>\n    <li>Variational + geometric perspective</li>\n    <li>Filtration optimization</li>\n    <li>Feature generation</li>\n  </ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Computation (time permitting)</p></li>\n  <ul>\n    <li>Implicit trace estimation</li>\n    <li>Stochastic Lanczos quadrature</li>\n    <li>matvecs are all you need</li>\n  </ul>\n  </div>\n</ul>\n\n\n## Interpretation: Regularization\n\n::: {.fragment fragment-index=1}\n\nIll-posed linear systems $Ax = b$ are often solved by \"regularized\" least-squares: \n\n$$\nx_\\tau^\\ast = \\argmin\\limits_{x \\in \\mathbb{R}^n} \\lVert Ax - b\\rVert^2 + \\tau \\lVert x \\rVert_1 \n$$\n\n:::\n\n::: {.fragment fragment-index=2}\n\nThe minimizer is given in closed-form by the regularized pseudo-inverse:\n\n$$\nx_\\tau^\\ast = (A^T A + \\tau I)^{-1} A^T b\n$$\n:::\n\n::: {.fragment fragment-index=3}\n\n![](images/lasso.png){width=50% fig-align=\"center\"}\n\n:::\n\n::: aside\n\nImage from: https://thaddeus-segura.com/lasso-ridge/\n\n::: \n\n## Interpretation: Regularization\n\n<br/>\n\n::: {.fragment fragment-index=1}\n\n\n<div style=\"text-align: center;\">\nUnder the appropriate parameters$^1$ for $\\nu$ and $p$, $\\phi$ takes the form:\n\n</div>\n\n$$\n\\phi(x, \\tau) = \\frac{2}{\\tau}\\int\\limits_{0}^z z \\cdot  \\big((z/\\sqrt{\\tau})^2+1\\big)^{-2} dz = \\frac{x^2}{x^2 + \\tau}\n$$\n\n:::\n\n\n::: {.fragment fragment-index=2}\n\n<div style=\"text-align: center;\">\n\nThe corresponding Löwner operator and its Schatten $1$-norm is given$^2$ by:\n\n</div>\n\n$$\n\\Phi_\\tau(X) = (X^T X + \\tau \\, I_n)^{-1} X^T X, \\quad \\quad \\lVert \\Phi_\\tau(X) \\rVert_\\ast = \\sum\\limits_{i = 1}^n \\frac{\\sigma_i(X)^2}{\\sigma_i(X)^2 + \\tau}\n$$\n\n:::\n\n::: {.fragment fragment-index=3}\n\n<div style=\"text-align: center;\">\n\nThis the <span style=\"color: purple;\"> _Tikhonov regularization_ </span> in standard form used in $\\ell_1$-regression (LASSO)\n\n</div>\n\n:::\n\n::: {.fragment fragment-index=4}\n\n<div style=\"text-align: center;\">\n\n$\\Leftrightarrow$ $\\tilde{\\beta}_p$ is a \"Tikhonov-regularized Betti number\"\n\n</div>\n\n:::\n\n::: aside \n\n\\(1\\) This $\\phi$ corresponds to setting $\\nu(\\tau) = \\sqrt{\\tau}$ and $p(x) = 2x (x^2 + 1)^{-2}$; \\(2\\) See Theorem 2 in @zhao2012approximation. \n\n::: \n\n\n## Application \\#1: Filtration optimization\n\n:::{layout=\"[[25,50,25]]\" layout-valign=\"bottom\"}\n\n<div class=\"column\" layout-align=\"right\">\n![](animations/dgms_vineyards.gif){width=100%}\n</div>\n\n<div class=\"column\" layout-align=\"center\">\n![](images/dgm_opt.png){height=100% fig-align=\"center\"}\n</div>\n\n<div class=\"column\" layout-align=\"left\">\n![](animations/complex_plain.gif){width=100%}\n</div>\n\n::::\n\n$$ \n\\alpha^\\ast = \\argmax_{\\alpha \\in \\mathbb{R}} \\; \\mathrm{card}\\big(\\, \\left.\\mathrm{dgm}(K_\\bullet, \\, f_\\alpha) \\right|_{R} \\, \\big) \n$$\n\n## Application \\#1: Filtration optimization\n\n:::{layout=\"[[60,30]]\" layout-valign=\"bottom\"}\n\n<div class=\"column\" layout-align=\"right\">\n![](images/codensity_mult.png)\n</div>\n\n<div class=\"column\" layout-align=\"center\">\n![](images/optimal_codensity_complex.png){width=100% height=100%}\n</div>\n\n:::\n\n$$ \n\\alpha^\\ast = \\argmax_{\\alpha \\in \\mathbb{R}} \\; \\mathrm{card}\\big(\\, \\left.\\mathrm{dgm}(K_\\bullet, \\, f_\\alpha) \\right|_{R} \\, \\big) \n$$\n\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex1.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \n\t\\mu_p^{R} = \n\t\\mathrm{rank}\\begin{bmatrix} \\partial_{p+1}^{j + 1, k} & 0 \\\\\n\t0 & \\partial_{p+1}^{i + 1, l}\n\t\\end{bmatrix}\n\t- \n\t\\mathrm{rank}\\begin{bmatrix} \\partial_{p+1}^{i + 1, k} & 0 \\\\\n\t0 & \\partial_{p+1}^{j + 1, l}\n\t\\end{bmatrix}\n$$\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex2.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \n\\mu_p^{R} = \n\\mathrm{tr}\\begin{bmatrix} \\lVert \\partial_{p+1}^{j + 1, k} \\rVert_\\ast & 0 \\\\\n0 & \\lVert \\partial_{p+1}^{i + 1, l} \\rVert_{\\ast}\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\lVert \\partial_{p+1}^{i + 1, k} \\rVert_\\ast & 0 \\\\\n0 & \\lVert  \\partial_{p+1}^{j + 1, l} \\rVert_\\ast\n\\end{bmatrix}\n$$\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex3.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \n\\hat{\\mu}_p^{R} = \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{j + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{i + 1, l})\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{i + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{j + 1, l})\n\\end{bmatrix}\n$$\n\n:::{.fragment }\n\n$$\\boxed{\\text{There exists a positive }\\tau^\\ast > 0 \\text{ such that } \\mu_p^R = \\lceil \\hat{\\mu}_p^R \\rceil \\text{ for all } \\tau \\in (0, \\tau^\\ast]}$$\n\n:::\n\n\n## Application \\#1: Filtration optimization {visibility=\"hidden\"}\n\n![](images/combinatorial_explosion.png){width=60%, fig-align=\"center\"}[^1]\n\n\n[^1]: Xu, Weiyu, and Babak Hassibi. \"Precise Stability Phase Transitions for $\\ell_1 $ Minimization: A Unified Geometric Framework.\" IEEE transactions on information theory (2011)\n\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex4.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \\mu_p^{R} = \\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{j + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{i + 1, l})\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{i + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{j + 1, l})\n\\end{bmatrix}\n$$\n\n## Application \\#1: Filtration optimization\n\n![](images/codensity_ex5.png){width=60%, fig-align=\"center\"}\n\n<br/>\n\n$$ \\mu_p^{R} = \\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{j + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{i + 1, l})\n\\end{bmatrix}\n- \n\\mathrm{tr}\\begin{bmatrix} \\Phi_\\tau(\\partial_{p+1}^{i + 1, k}) & 0 \\\\\n0 & \\Phi_\\tau(\\partial_{p+1}^{j + 1, l})\n\\end{bmatrix}\n$$\n\n:::{.fragment style=\"text-align: center\"}\n\nSimilar to the Iterative Soft-Thresholding Algorithm (ISTA) [@beck2017first]\n\n:::\n\n\n## Interpretation: Diffusion\n\n<hr/>\n\n::: {.fragment fragment-index=1 style=\"text-align: center\"}\n\nDiffusion processes on graphs often modeled as time-varying $v(t) \\in \\mathbb{R}^n$ via:\n\n$$ v'(t) = -L v(t) \\quad \\Leftrightarrow \\quad L \\cdot u(x,t) = - \\frac{\\partial u(x, t)}{\\partial t} $$\n\n::: \n\n::: {.fragment fragment-index=2 layout-valign=\"center\" style=\"text-align: center\"}\n\n![](images/diffusion2.png){width=90% fig-align=\"center\"}\n\n:::\n\n::: {.fragment fragment-index=3 style=\"text-align: center\"}\n\nValue of $v(t)$ at time $t$ given by the _Laplacian exponential diffusion kernel_:\n\n$$v(t) = \\mathrm{exp}(-t L) v(0)$$\n\n<!-- $$H_t = U \\mathrm{exp}(-t \\Lambda) U' = \\sum\\limits_{i=1}^n e^{-t \\lambda_i} \\, u_i \\, u_i^T$$ -->\n\n:::\n\n:::{.aside style=\"margin-top: 2em !important;\"}\n\nImages from @crane2017heat and @sharma2011topologically\n\n::: \n\n\n## Interpretation: Diffusion\n\n<hr/>\n\n::: {.fragment fragment-index=1}\n\n<div style=\"text-align: center;\">\n\nUnder the appropriate parameters for $\\nu$ and $\\rho$^[This $\\phi$ corresponds to setting $\\nu(\\tau) = \\tau$ and $p(x) = \\mathrm{exp}(-x)$ for $x > 0$ and $p(x) = 0$ otherwise], $\\phi$ takes the form:\n\n</div>\n\n$$\n\\phi(x, \\tau) = 1 - \\mathrm{exp}(- x / \\tau)\n$$\n\n:::\n\n\n::: {.fragment fragment-index=3}\n\n<div style=\"text-align: center;\">\n\nThe corresponding Löwner operator and its Schatten $1$-norm is given by (for $t = \\tau^{-1}$):\n\n</div>\n\n$$\n\\Phi_\\tau(X) \\simeq U \\mathrm{exp}(-t \\Lambda) U^T, \\quad \\mathrm{tr}(\\Phi_\\tau(X)) \\simeq \\sum\\limits_{i = 1}^n \\mathrm{exp}(-t \\cdot \\lambda_i)\n$$\n\n:::\n\n::: {.fragment fragment-index=4}\n\n<div style=\"text-align: center;\">\n\nThis is the <span style=\"color: red;\"> _Heat kernel_ </span> and its Schatten-1 norm is the <span style=\"color: red;\"> _heat kernel trace_ </span>\n\n</div>\n\n:::\n\n<br> \n\n::: {.fragment fragment-index=5}\n\n<div style=\"text-align: center; border: 1px;\">\n\nBoth quantities proven useful in crafting _geometric signatures_$^2$\n\n</div>\n\n:::\n\n::: aside\n\nSee e.g. @sun2009concise, @bronstein2010scale, and @xiao2009graph\n\n:::\n\n## Application \\#2: Featurization\n\n<hr/>\n\n:::{.fragment style=\"text-align: center\"}\n\nConsider filtering a fixed $K$ embedded in $R^d$ by a 1-parameter directions in $S^{d-1}$\n\n$$\nK_\\bullet = K(v)_i = \\{\\, x \\in X \\mid \\langle x, v \\rangle \\leq i  \\,\\}\n$$\n\n:::\n\n:::{.fragment style=\"text-align: center\"}\n\n![](animations/dt_single.gif){width=350 height=100% fig-align=\"center\"}\n\n:::\n\n<!-- ![](images/dt.png){width=400 height=100% fig-align=\"center\"} -->\n\n## Application \\#2: Featurization\n\n<hr/>\n\n$$\nK_\\bullet = K(v)_i = \\{\\, x \\in X \\mid \\langle x, v \\rangle \\leq i  \\,\\}\n$$\n\n![](animations/ph_transform.gif){width=550 height=100% fig-align=\"center\"}\n\n\n:::{.fragment}\n\n$$\\{ \\; \\mathrm{dgm}(v) : v \\in S^{d-1} \\; \\} \\Leftrightarrow \\text{Persistent Homology Transform (PHT)}$$ \n\n:::\n\n:::{.fragment style=\"text-align: center;\"}\n\nTurner et al.$^1$ show PHT(X) is injective, sparking an inverse theory for persistence!\n\n:::\n\n::: aside \n\n\\(1\\): @turner2014persistent\n\n:::\n\n## Application \\#2: Featurization\n\n<hr/>\n\n:::{.fragment .fade-in-then-semi-out}\n\nInjectivity of the PHT $\\implies$ can impose metric over <span style=\"color: orange;\"> _shape space_ </span> by integrating $d_B$:\n\n$$ \\mathcal{D}_{PHT}(X, Y) \\triangleq \\sum_{p=0}^d \\int_{S^{d-1}} \\operatorname{d}_B\\left(\\mathrm{dgm}_p(X, v) \\right), \\left( \\mathrm{dgm}_p(Y, v) \\right) dv $$\n\n:::\n\n<!-- :::{.fragment layout=[[50,50]]}\n\n![](images/turtle1.png){width=650 height=100% fig-align=\"right\"}\n\n![](images/turtle2.png){width=650 height=100% fig-align=\"left\"}\n\n::: -->\n\n<!-- :::{.fragment .fade-in-then-semi-out}\nTo make $\\mathcal{D}(\\cdot, \\cdot)$ blind to rotations, Turner^[@turner2014persistent] minimize $\\mathcal{D}$ over rotations $\\{R_i\\}_{i=1}^m$:\n\n$$ d_{\\mathrm{PHT}}(X, Y) = \\inf_{i = 1, \\dots, m} \\mathcal{D}(X, R_i(Y)) $$\n\n::: -->\n\n\n:::{.fragment .fade-in style=\"text-align: center;\"}\n\n<!-- When $m = \\lvert V \\rvert$, computing $d_{\\mathrm{PHT}}(X, Y)$ requires:  -->\n\n<ol>\n  <li> Computing $\\mathrm{dgm}_p(\\cdot, v)$ for $\\{X,Y\\}$ over $\\{v_i\\}_{i=1}^m \\subset S^{d-1}$ ( ${\\color{orange} O(m \\cdot N^3)}$ )</li>\n  <li> Minimizing $\\mathcal{D}$ over all $m$ rotations ( ${\\color{red}\\approx O(m^2 \\cdot N^{1.5} \\log N)}$ )$^1$ </li>\n</ol>\n\n:::\n\n<!-- <ol>\n  <li> Computing $\\mathrm{tr}(\\phi_\\tau(\\cdot))$ for $\\{X,Y\\}$ over sufficiently dense $\\mathcal{V} \\subset S^{d-1}$ ($\\approx {\\color{blue} O(m \\cdot N^2)}$)</li>\n  <li> Phase-aligning two _periodic_ signals via FFT ( ${\\color{green} O(m \\log m)}$ ) </li>\n</ol> -->\n\n:::{.fragment .fade-in }\n$${\\Large \\text{ This can be very expensive to do! }}$$\n\n:::\n\n::: aside \n\\(1\\) Assumes $d_B \\sim O(n^{1.5} \\log n)$, following [@kerber2017geometry]. \n:::\n\n## Application \\#2: Featurization \n\n<hr/>\n\nInformativity of Heat Kernel $\\implies$ can impose _pseudo_-metric over <span style=\"color: orange;\"> _shape space_</span>:\n\n$$ \\mathcal{D}_{HK}(X, Y) \\triangleq \\sum_{p=0}^d \\int_{S^{d-1}} \\lVert \\mu_p^R(X, v) - \\mu_p^R(Y, v) \\rVert_2 $$\n\n<!-- $$ \\mathcal{D}(X, Y) \\triangleq \\sum_{p=0}^d \\int_{S^{d-1}} \\operatorname{d}_B\\left(\\mathrm{dgm}_p(X, v) \\right), \\left( \\mathrm{dgm}_p(Y, v) \\right) dv $$ -->\n\n\n<!-- When $m = \\lvert V \\rvert$, computing $d_{\\mathrm{PHT}}(X, Y)$ requires:  -->\n<!-- $\\mathrm{tr}(\\phi_\\tau(\\cdot))$ $\\xcancel{d_{\\mathrm{PHT}}(X, Y)}$ requires:  -->\n\n<!-- <ol color=\"gray;\">\n  <li> Computing $\\mathrm{dgm}_p(\\cdot, v)$ for $\\{X,Y\\}$ over sufficiently dense $\\mathcal{V} \\subset S^{d-1}$ ( ${\\color{orange} O(m \\cdot N^3)}$ )</li>\n  <li style=\"text-style: strikethrough;\"> Minimizing $\\mathcal{D}$ over all $m$ rotations ( ${\\color{red}\\approx O(m^2 \\cdot N^{1.5} \\log N)}$ )$^1$ </li>\n</ol> -->\n\n<!-- When $m = \\lvert V \\rvert$, computing $\\mathrm{tr}(\\phi_\\tau(\\cdot))$ $\\xcancel{d_{\\mathrm{PHT}}(X, Y)}$ requires:  -->\n\n:::{style=\"text-align: center;\"}\n\n<ol>\n  <li> Computing $\\mu_p^R(\\cdot, v)$ for $\\{X,Y\\}$ over $\\{v_i\\}_{i=1}^m \\subset S^{d-1}$ ($\\approx {\\color{blue} O(m \\cdot N^2)}$)</li>\n  <li> Phase-aligning two _periodic_ signals via FFT ( ${\\color{green} O(m \\log m)}$ ) </li>\n</ol>\n\n:::\n\n:::{layout=[[36,62]] style=\"text-align: center;\"}\n\n![](animations/dt_single.gif){height=270px fig-align=\"right\"}\n\n![](animations/trace_summary.gif){height=275px fig-align=\"left\"}\n\n:::\n\n## Application \\#2: Featurization\n\n\n\n![](images/shape_signatures.png){width=850 height=100% fig-align=\"center\"}\n\n<!-- :::{layout=[[50,50]]}\n\n![](images/turtle1.png){width=650 height=100% fig-align=\"right\"}\n\n![](images/bone1.png){width=650 height=100% fig-align=\"left\"}\n\n::: -->\n\n\n## Application \\#3: Topology-guided sparsification \n\n![](images/elephant_sparsify.png){fig-align=\"left\" style=\"max-width: 1150px !important; max-height: 1150px !important;\"}\n\n\n## Overview \n\n<ul>\n  <div style=\"color: #7F7F7F;\"> \n  <li>Theory</li>\n  <ul>\n    <li>Rank duality</li>\n    <li>Spectral functions</li>\n    <li>Operator properties</li>\n  </ul>\n  </div>\n  <div style=\"color: #7F7F7F;\"> \n  <li><p>Applications</p></li>\n  <ul>\n    <li>Variational + geometric perspective</li>\n    <li>Filtration optimization</li>\n    <li>Feature generation</li>\n  </ul>\n  </div>\n  <li><p>Computation (time permitting)</p></li>\n  <ul>\n    <li>Implicit trace estimation</li>\n    <li>Stochastic Lanczos quadrature</li>\n    <li>matvecs are all you need</li>\n  </ul>\n</ul>\n\n\n\n## Beyond $\\mathrm{dgm}$'s: Revisiting the rank computation {visibility='hidden'}\n \n$$ \\beta_p^{i,j} : \\mathrm{rank}(H_p(K_i) \\to H_p(K_j))$$\n\t\n<!-- <hr style=\"margin: 0; padding: 0;\">  -->\n\n:::{.incremental style=\"list-style-type: none;align=center;\"}\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n$\\;\\quad\\quad\\quad\\beta_p^{i,j} = \\mathrm{dim} \\big( \\;\\mathrm{Ker}(\\partial_p(K_i))\\; / \\;\\mathrm{Im}(\\partial_{p+1}(K_j)) \\; \\big )$\n:::\n\n<!-- <li style=\"text-align: left\" class=\"fragment fade-in-then-semi-out\">\n$\\quad\\quad\\quad\\quad\\beta_p^{i,j} = \\mathrm{dim} \\big( \\;\\mathrm{Ker}(\\partial_p(K_i))\\; / \\;\\mathrm{Im}(\\partial_{p+1}(K_j)) \\; \\big )$\n</li> -->\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n<!-- <li style=\"text-align: left\" class=\"fragment fade-in-then-semi-out\"> -->\n$\\;\\quad\\quad\\quad\\hphantom{\\beta_p^{i,j} }= \\mathrm{dim}\\big(\\; \\mathrm{Ker}(\\partial_p(K_i)) \\; / \\; (\\mathrm{Ker}(\\partial_p(K_i)) \\cap \\mathrm{Im}(\\partial_{p+1}(K_j))) \\; \\big )$\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n$\\;\\quad\\quad\\quad\\hphantom{\\beta_p^{i,j}}=\\color{blue}{\\mathrm{dim}\\big(\\;\\mathrm{Ker}(\\partial_p(K_i)) \\; \\big)} \\; \\color{black}{-} \\; \\color{red}{\\mathrm{dim}\\big( \\; \\mathrm{Ker}(\\partial_p(K_i)) \\cap \\mathrm{Im}(\\partial_{p+1}(K_j))\\;\\; \\big)}$\n:::\n\n::: \n\n<!-- <br>  -->\n::: {.fragment .fade-in-then-semi-out}\nRank-nullity yields the <span style=\"color: blue\">left term</span>: \n$$\n\\mathrm{dim}\\big(\\mathrm{Ker}(\\partial_p(K_i))\\big) = \\lvert C_p(K_i) \\rvert - \\mathrm{dim}(\\mathrm{Im}(\\partial_p(K_i)))\n$$\n:::\n\n:::{.fragment .fade-in-then-semi-out}\n<!-- Computing the <span style=\"color: red\">right term</span> more nuanced:  -->\n\"Relaxing\" the <span style=\"color: red\">right term</span> poses some difficulties:\n:::\n\n:::{.incremental style=\"list-style-type: none; align=center; text-align: left; margin-left: 2.5em; margin: 0; padding: 0;\"}\n- Pseudo-inverse$^1$, projectors$^2$, Neumann's inequality$^3$, etc.\n- PID algorithm$^4$, Reduction algorithm$^5$, Persistent Laplacian$^6$\n:::\n\n:::{.aside}\n@anderson1969series, @ben1967geometry, @ben2015projectors, @zomorodian2004computing, @edelsbrunner2000topological, @memoli2022persistent\n:::\n\n\n## Computation in quadratic time {visibility=\"hidden\"}\n\n<hr/> \n\n<div class=\"incremental\" style=\"list-style-type: none; align=left;\">\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\nComputing $A = U \\Lambda U^T$ for any $A \\in \\mathbf{S}_+^n$ bounded by $\\Theta(n^3)$ time and $\\Theta(n^2)$ space^[Assumes the standard matrix multiplication model for simplicity (i.e.  excludes the Strassen-family)]\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\n<u>However</u>, if $v \\mapsto Av \\approx O(n)$, then $\\Lambda(A)$ obtainable in <span style=\"color: red;\"> $O(n^2)$ time </span> and <span style=\"color: red;\">$O(n)$ space</span>!\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\n__Idea__: For some random $v \\in \\mathbb{R}^n$, expand successive powers of $A$:\n\n$$ \n\\begin{align}\nK_j &= [ v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{j-1}v] && \\\\\nQ_j &= [ q_1, q_2, \\dots, q_j] \\gets \\mathrm{qr}(K_j) && \\\\\nT_j &= Q_j^T A Q_j &&\n\\end{align}\n$$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\nIt turns out that every $A \\in \\mathbf{S}$ expanded this way admits a _three-term recurrence_ \n\n$$ A q_j = \\beta_{j-1} q_{j-1} + \\alpha_j q_j + \\beta_j q_{j+1} $$\n\n:::\n\n::: {.fragment .fade-in-then-semi-out style=\"text-align: left\"}\n\n<div style=\"text-align: center; font-size: 34px;\"> \n\nThis is the renowned *__Lanczos method__* for Krylov subspace expansion\n\n</div>\n\n<!-- <img src=\"images/lanczos_top_10.png\" style=\"position: fixed; top: 50%; left: 50%; transform: translate(-50%, -65%); height: 65vh !important; width: 95%;\"> </img> -->\n\n\n:::\n\n</div>\n\n## Lanczos iteration {visibility=\"hidden\"}\n\n![](animations/lanczos_krylov.gif){width=75% height=100% fig-align=\"center\"}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 0em; \">\n__Theorem [@simon1984analysis]__: Given a symmetric rank-$r$ matrix $A \\in \\mathbb{R}^{n \\times n}$ whose matrix-vector operator $A \\mapsto A x$ requires $O(\\eta)$ time and $O(\\nu)$ space, the Lanczos iteration computes $\\Lambda(A) = \\{ \\lambda_1, \\lambda_2, \\dots, \\lambda_r \\}$ in $O(\\max\\{\\eta, n\\}\\cdot r)$ time and $O(\\max\\{\\nu, n\\})$ space _when executed in exact arithmetic_. \n</div>\n\n## Implicit trace estimation\n\n<!-- Spectral-sum computation reduces to  _implicit matrix trace problem_ -->\n\n> __Implicit trace problem__: Given access to a square $A \\in \\mathbb{R}^{n \\times n}$ via its matrix–vector product operation $v \\mapsto Av$, estimate $\\mathrm{tr}(A) = \\sum_{i=1}^n A_{ii}$\n\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n![](images/black_box_trace.png){width=75% height=100% fig-align=\"center\"}\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nExact solution uses $O(n)$ matvecs $\\mathrm{tr}(A) = \\sum e_i^T A e_i$. \n\n:::\n\n<!-- ::: aside \n\n\\(1\\). @epperly2023xtrace @ghorbani2019investigation\n\n::: -->\n\n\n## _Randomized_ implicit trace estimation\n\n<hr/>\n\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nLet $A = \\mathbb{R}^{n \\times n}$. If $v \\in \\mathbb{R}^n$ a $\\mathrm{r.v.}$ with $\\mathbb{E}[vv^T] = I$, then: \n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: left;\"}\n\n$$ \\mathtt{tr}(A) = \\mathtt{tr}(A \\mathbb{E}[v v^T]) =  \\mathbb{E}[\\mathtt{tr}(Avv^T)] = \\mathbb{E}[\\mathtt{tr}(v^T A v)] = \\mathbb{E}[v^T A v] $$\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n$$\n\\implies \\mathtt{tr}(A) \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i, \\quad \\text{ for } v_i \\sim \\{-1, +1\\}^n\n$$\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 0em; \">\n__Theorem [@hutchinson1989stochastic]__: For any $A \\in S_+^n$, if $n_v \\geq (6/\\epsilon^2) \\log(2/\\eta)$ unit-norm $v \\in \\mathbb{R}^n$ are drawn uniformly from $\\{-1, +1\\}^n$, then $\\forall \\; \\epsilon, \\eta \\in (0,1)$:\n$$ \\mathrm{Pr}\\bigg(\\lvert \\mathrm{tr}_{n_v}(A) - \\mathrm{tr}(A) \\rvert \\leq \\epsilon \\cdot \\mathrm{tr}(A) \\bigg) \\geq 1 - \\eta$$\n\n</div>\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nSuch an estimator $\\mu_{n_v} \\triangleq \\mathrm{mean}(\\{ v_i^T A v_i \\}_{i=1}^{n_v})$ is called a _Girard-Hutchinson_ estimator\n\n:::\n\n## Randomized trace approximation \n\n<hr/> \n\n\n<!-- :::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n$\\Phi_\\tau(X)$ are _trace-class_, satisfy $\\mathtt{tr}(\\Phi_\\tau(X)) = \\sum\\limits_{i=1}^n \\phi(\\lambda_i, \\tau)$\n\n::: -->\n\n::::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nUbaru$^1$ showed $\\mu_{n_v}$ generalizes to _matrix functions_ via __stochastic Lanczos quadrature__\n\n$$ \\mathrm{tr}(\\Phi_\\tau(X)) \\approx \\frac{n}{n_v} \\sum\\limits_{i=1}^{n_v} \\bigg( e_1^T \\, \\Phi_{\\tau}(T_i) \\, e_1 \\bigg ), \\quad \\quad T_i = \\mathrm{Lanczos}(X, v_i) $$\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nThe degree-$k$ Lanczos takes $O(\\eta \\cdot k^2)$ time, where $\\eta$ is the time to do $v \\mapsto Xv$\n<!-- $$ \\hat{\\mu}_p^{R} \\sim T(n^2 \\cdot s l^2) \\approx O(n^2) \\text{ time, where } n \\sim \\lvert K^p \\rvert \\text{ and } l, s \\sim O(1)^{1} $$  -->\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\nFor $p$-Laplacians, the complexity of $v \\mapsto L_p v$ is $\\approx O(m)^1$ where $m = \\lvert K_{p+1}\\rvert$\n\n:::\n\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n\n$\\implies$ can compute $\\epsilon$-approx.$^2$ of $\\mu_p^{\\ast}$ or $\\beta_p^{\\ast}$ with success probability $1 - \\eta$ in: \n\n$$ O((m / \\epsilon^{2}) \\log(\\eta^{-1})) \\text{ time}, \\quad O(m) \\text{ space }$$\n\n:::\n\n:::{.fragment .fade-in-then-semi-out style=\"text-align: center\"}\n<hr/>\n\n$${\\Large \\text{ Matvecs are all you need! }}$$\n\n<hr/>\n:::\n\n::: aside \n\n\\(1\\) Assumes $p$ is small, fixed constant. \n\\(2\\) Assumes the Lanczos degree $k$ is constant. In practice, $k \\leq 20$ is typically sufficient.\n\n:::\n\n## Brief history of spectral sum estimation \n\nThis is an active area. Sampling of seminal work, improvements, and major applications:\n\n<div style=\"font-size: 1.425rem; text-align: center;\"> \n\n- \\(1950\\) Lanczos publishes \"method of minimized iterations\" for tridiagonalization\n- \\(1969\\) Golub + Welsch study Gaussian quadrature (GQ) rules \n- \\(1989\\) Hutchinson proposes unbiased estimator for $\\mathrm{tr}(A)$ based on QFs\n- \\(2000\\) Estrada proposes trace-based index quantifying \"degree of folding\" in proteins\n- \\(2009\\) Golub elucides connections between orthogonal polynomials, GQ, and \"moment matching\"\n- \\(2017\\) Ubaru proposes _stochastic Lanczos quadrature_: Hutchinson estimator + Lanczos quadrature\n- \\(2017\\) Musco finds Lanczos in FP is optimal for $v \\mapsto f(A)v$ approx.\n- \\(2019\\) Hessian eigenvalue density found crucial to ImageNet performance (est. w/ trace)\n- \\(2021\\) Hutch++ proposed by Musco, achieves optimal $1/m^2$ variance reduction \n- \\(2021\\) Adaptive variant of Hutch++ given that reduces query complexity\n- \\(2021\\) AdaHessian substantially improves Adam performance on ResNet32 + Cifar10\n- \\(2023\\) Epperly uses exchangability to produce family of minimum-variance $\\mathrm{tr}$ estimators\n</div>\n\n## Scalability: $\\mathrm{matvecs}$'s are all you need\n\n![](images/imate_trace_bench.png){width=750 height=100% fig-align=\"center\"}\n\nFigure taken from the new `imate` package documentation ([`[gh]/ameli/imate`](https://ameli.github.io/imate/index.html))\n\n\n## Application \\#3: Computing $\\mathrm{dgm}$'s\n\n\n:::{layout=\"[[50,50]]\" layout-valign=\"bottom\"}\n\n![](images/divide_conquer_dgm.png){width=400 height=100% fig-align=\"right\"}\n\n![](images/bisection_tree.png){width=400 height=100% fig-align=\"left\"}\n\n:::\n\n:::{.fragment}\n\n<div style=\"padding-left: 1em; border: 1px solid black; margin: 0em; \">\n__Theorem [@chen2011output]__: For a simplicial complex $K$ of size $\\lvert K \\rvert = n$, computing the $\\Gamma$-persistent pairs requires $O(C_{(1-\\delta)\\Gamma}\\mathrm{R}(n) \\log n)$ time and $O(n + \\mathrm{R}(n))$ space, where $R(n)$ ($R_S(n)$) is the time (space) complexity of computing the rank of a $n \\times n$ boundary matrix.\n</div>\n\n:::\n\n\n<!-- ## Conclusion \n\nSpectral relaxation of rank invariant using _matrix functions_ \n\n- Suitable for parameterized families of filtrations\n- Differentiable + amenable for optimization \n- Stable to perturbations in $f_\\alpha$ when $\\tau > 0$ \n- Excellent compute properties. Implementation ongoing. \n- Better optimizer implementation also ongoing.  -->\n\n<!-- Looking for collaborators + ideas! In particular: -->\n\n<!-- :::{.incremental}\n\n- Optimizing parameterized filtrations\n- Differentiating n-parameter families of filtrations\n- Encoding features with Laplacian spectra\n- Sparse minimization problems (compressive sensing)\n- Understanding connections to other areas of math\n\n::: -->\n\n## Application: Manifold detection (time permitting) {visibility=\"hidden\"}\n\n:::{layout=\"[[50,50]]\" layout-valign=\"bottom\"}\n\n![](images/patch_manifold.png){width=90% fig-align=\"right\"}\n\n![](images/klein_hilbert.png){width=90% fig-align=\"left\"}\n\n:::\n\n\n## Other applications (time permitting) {visibility=\"hidden\"}\n\n![](images/dw_chi_comp.png){width=50% height=90% fig-align=\"center\"}\n\n![](images/elephant_sparsify.png){width=80% height=100% fig-align=\"center\"}\n\n\n## Acknowledgements & Advertising\n\n- Explicit proof of (unfactored) $\\beta_p^{\\ast}$ in CT book by Tamal Dey & Yusu Wang \n- Expression + PH algorithm involving $\\mu_p^\\ast$ by Chao Chen & Michael Kerber \n- Insightful developments by [@bauer2022keeping] and [@memoli2022persistent]\n- SLQ due to @ubaru2017fast; implementation available @ [gh/peekxc/primate](https://peekxc.github.io/primate/)\n<hr/>\n\n<div style=\"text-align: center; font-size: 40px;\"> \n\nInfo: [[gh]/peekxc](https://github.com/peekxc) or [mattpiekenbrock.com](https://mattpiekenbrock.com/)\n\nPlug: I'm looking for a job this year!\n\n</div>\n\n<hr/>\n\n<div style=\"text-align: center; font-size: 80px;\"> \n\nThanks for listening! \n\n</div>\n\n## References\n\n::: {#refs}\n:::\n\n<script>\n  window.WebFontConfig = {\n    custom: {\n      families: ['KaTeX_AMS'],\n    },\n  };\n</script>\n\n\n\n## Computation {visibility=\"hidden\"}\n\n:::{.fragment}\n\n- Permutation invariance $\\implies$ can optimize memory access of $\\mathtt{SpMat}$ operation\n\n:::\n\n:::{.fragment}\n\n- Any complex data structure suffices, e.g. tries$^2$, combinadics, etc...\n\n::: \n\n:::{.fragment}\n\n- Iterative Krylov methods / Lanczos dominate solving sparse systems$^2$\n\n::: \n\n:::{.fragment}\n\n- Many laplacian preconditioning methods known [@jambulapati2021ultrasparse]\n\n::: \n\n:::{.fragment}\n\n- Nearly optimal algorithms known for SDD [@stathopoulos2007nearly]\n\n::: \n\n:::{.aside}\n\nSee [@komzsik2003lanczos, @parlett1995we] for an overview of the Lanczos. See [@boissonnat2014simplex] for representing complexes.\n\n:::\n\n\n\n\n## Why not just use diagrams?\n\n<!-- Extending the reduction algorithm to parameterized settings is highly non-trivial -->\n\n:::: {.columns}\n\n:::{.fragment}\n\n::: {.column width=\"45%\" layout-align=\"right\" style=\"margin-left: 1em;\"}\n![](animations/dgms_vineyards.gif){width=90%}\n:::\n\n::: {.column width=\"45%\" height=\"1em\" layout-align=\"left\"}\n![](animations/complex_plain.gif){width=90%}\n:::\n\n:::\n\n::::\n\n:::{style=\"text-align: center;\"}\n\n__Pro:__ Diagrams are stable, well-studied, and information rich.\n\n:::\n\n\n::: {.notes}\nReduction algorithm is a restricted form of gaussian elimination. \n:::\n\n\n## Why not just use diagrams?\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n![](animations/dgms_vineyards.gif){width=90%}\n:::\n\n::: {.column width=\"33%\" height=\"1em\"}\n![](animations/complex.gif){width=90%}\n:::\n\n::: {.column width=\"33%\"}\n![](animations/spy_matrices.gif){width=90%}\n:::\n\n::::\n\n:::{style=\"text-align: center;\"}\n\n__Con:__ Extending the $R = \\partial V$ to parameterized settings is non-trivial\n\n:::\n\n<div style=\"text-align: center;\">\n\nMaintaining the $R = \\partial V$ decomposition \"across time\" $\\implies$ huge memory bottleneck\n\n</div>\n\n:::{.aside}\nFor details, see @piekenbrock2021move or @bauer2022keeping (also @lesnick2015interactive)\n:::\n\n::: {.notes}\nReduction algorithm is a restricted form of gaussian elimination. \n:::\n\n\n\n## Computation {visibility=\"hidden\"}\n\nPreliminary experiments suggest scalability is promising \n\n![](images/watts_strogatz_perf.png)\n\n- $\\approx \\, \\leq 25$ Lanczos vectors to approximate full spectrum at any $\\epsilon > 0$\n- $\\implies O(n)$ memory to obtain $\\lVert \\cdot \\rVert_\\ast$ in $O(n^2)$ time (with small constants!)\n- Previously computed eigenvectors can be re-used for \"warm restarts\"\n\n## Experiment \\#1: Directional Transform  {visibility=\"hidden\"}\n\n![](images/shape_signatures.png){width=80% height=100% fig-align=\"center\"}\n\n:::{.notes}\nLuis mentioned modding out rotations and translations adn scale to compare shapes. We can handle rotations via phase alignment. \n\nSarah mentioned handling orientation.\n\n:::\n\n\n## Permutation Invariance {visibility=\"hidden\"}\n\nConsider the setting where $f_\\alpha : \\mathbb{R} \\to \\mathbb{R}^N$ is an $\\alpha$-parameterized filter function: \n\n$$ \\mu_p^R(\\, f_\\alpha \\, ) = \\{ \\mu_p^R(K_\\bullet^\\alpha) : \\alpha \\in \\mathbb{R} \\}$$\n\nDifficult to compute $R_\\alpha = \\partial_\\alpha V_\\alpha$ for all $\\alpha$ as $K_\\bullet = (K, f_\\alpha)$ is changing constantly...\n\n<!-- #\\mathrm{rank}(\\partial_p(K_\\bullet( \\, f_\\alpha \\,))) -->\n\n<!-- On the other hand... -->\n$$ \\mathrm{rank}(\\partial_p(K_\\bullet)) \\equiv \\mathrm{rank}(P^T \\partial_p(K) P) $$\n$$ \\mathrm{rank}(\\partial_p(K_\\bullet)) \\equiv \\mathrm{rank}(W \\mathrm{sgn}(\\partial_p(K)) W) $$\n\nThus we may decouple $f_\\alpha$ and $K$ in the computation: \n\n$$\n\\begin{align*}\n \\mu_p^{R}(K,f_\\alpha) &\\triangleq \\mathrm{rank}\\big(\\,\\hat{\\partial}_{q}^{j + \\delta, k}\\,\\big) - \\; \\dots \\; + \\mathrm{rank}\\big(\\, \\hat{\\partial}_{q}^{i + \\delta, l}\\,\\big)  \\\\\n&\\equiv \\mathrm{rank}\\big(\\,V_p^j \\circ \\partial_{q} \\circ W_q^k \\,\\big) - \\; \\dots \\; + \\mathrm{rank}\\big(\\,V_p^{i+\\delta} \\circ \\partial_{q} \\circ W_q^l \\,\\big)\n \\end{align*}\n $$\n\nwhere the entries of $V$, $W$ change continuously w/ $\\alpha$, while $\\partial_q$ remains _fixed_...\n\n## Spectral functions {visibility=\"hidden\"}\n\nNuclear norm $\\lVert X \\rVert_\\ast = \\lVert \\mathbf{\\sigma} \\rVert_1$ often used in sparse minimization problems like _compressive sensing_ due to its convexity in the unit-ball $\\{A \\in \\mathbb{R}^{n \\times m} : \\lVert A \\rVert_2 \\leq 1 \\}$\n\n:::{layout=\"[[50,50]]\" layout-valign=\"bottom\"}\n\n![](images/l0_l1.png){width=300 height=100% fig-align=\"right\"}\n\n![](images/convex_envelope.png){width=320 height=100% fig-align=\"left\"}\n\n:::\n\n<div style=\"text-align: center;\"> \n\n__Left:__ The $\\ell_0$ and $\\ell_1$ norms on the interval $[-1,1]$\n\n__Right:__ $g$ forms the convex envelope of $f$ in the interval $[a,b]$\n\n</div> \n\n## Spectral functions {visibility=\"hidden\"}\n\nUnfortunately, $\\lVert \\cdot \\rVert_\\ast$ often a poor substitute for rank\n\n![](images/rank_relax.png){width=70% height=100% fig-align=\"center\"}\n\n__Left:__ The $\\ell_0$ and $\\ell_1$ norms on the interval $[-1,1]$\n__Right:__ \n\n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\n__Dataset:__ 3D meshes of animals in different poses [@chazal2009gromov]\n\n</div>\n\n![](images/gh_data_pose.png){width=425 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\n__Challenge:__ Recognize intrinsic shape categories (via a distance metric)\n\n</div>\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\nThe _Gromov-Hausdorff_ distance yields a metric on the set of compact metric spaces $\\mathcal{X}$\n\n$$\nd_{GH}(d_X, d_Y) = \\sup_{x \\in X, y \\in Y} \\lvert d_X(x, \\psi(y)) - d_Y(y, \\phi(x))\\rvert \n$$\n\n![](images/camel_gh.png){width=600 height=100% fig-align=\"center\"}\n\nUsing intrinsic metric makes $d_{\\mathrm{GH}}$ blind to e.g. shapes represented in different _poses_\n\n:::{.fragment}\n\n<div style=\"text-align: center;\"> \n\nUnfortunately, the GH distance is NP-hard to compute [@memoli2012some]\n\n</div>\n\n:::\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\nIt's known $d_B$ ($d_W$) on Rips filtrations $\\mathcal{R}(X, d_X)$ lower bound GH (GW) distance\n\n</div> \n\n$$ \nd_B(\\mathrm{dgm}_p(\\mathcal{R}(X, d_X)), \\mathrm{dgm}_p(\\mathcal{R}(Y, d_Y))) \\; \\leq \\; d_{GH}((X, d_X), (Y, d_Y))\n$$\n\n:::{layout-ncol=1}\n\n![](images/camel_gh_rips_comparison.png){width=875 height=100% fig-align=\"center\"}\n\n:::\n\n<div style=\"text-align: center;\"> \n\nMotivates use of persistence in metric settings for e.g. shape classification!\n\n</div> \n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\n__Issue:__ Diagrams are far from injective, cannot distinguish e.g. stretched shapes\n\n</div> \n\n![](images/dgm_noninjective.png){width=575 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\nThe lower bound on $d_{\\mathrm{GH}}$ could be totally useless!\n\n</div> \n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n:::{.fragment}\nLower bounds extend to Rips filtrations _augmented_ with real-valued functions $f, g$: \n\n$$\n\\mathcal{R}(f) \\triangleq \\mathcal{R}(X, d_X, f) = \\{\\mathcal{R}_\\alpha(X_\\alpha)\\}_{\\alpha > 0}, \\quad X_\\alpha \\triangleq f^{-1}((-\\infty, \\alpha)) \\subseteq X\n$$\n\n:::\n\n:::{.fragment}\n\nThe diagrams from $\\mathcal{R}(\\lambda \\cdot f_X)$ represent _stable signatures_ for each $\\lambda > 0$:\n\n$$\nd_B(\\mathcal{R}(\\lambda \\cdot f_X), \\mathcal{R}(\\lambda \\cdot f_Y)) \\leq \\max(1, \\lambda L) \\cdot d_{\\mathrm{GH}}(X, Y)\n$$\n\n:::\n\n<!-- <div style=\"text-align: center;\">  -->\n\n<!-- </div>  -->\n\n<!-- <hr>  -->\n\n:::{.fragment}\n\nChazal showed these bounds extend to metrics on _augmented_ metric spaces:\n\n$$\n\\mathcal{X}_1 = \\{ (X, d_X, f_X) \\mid (X, d_X, f_X) \\in \\mathcal{X}, f_X: X \\to \\mathbb{R} \\text{ continuous }\\}\n$$\n\nThese signatures also extend to measure metric spaces, see [@chazal2009gromov]\n\n::: \n\n:::{.fragment}\n\n<br>  \n\n<span style=\"color: red;\"> NOTE:   </span> \nSize of $L$ depends on the choice of $f$ + each $\\lambda$ produces a new signature! \n\n:::\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\n__Ex:__ The _eccentricity_ function $e_X^1(x) = \\max_{x' \\in X} d_X(x,x')$ has $L = 2$ \n\n</div> \n\n![](images/dgm_noninjective2.png){width=775 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\nAugmenting via a fraction of $e_X^1$ modifies the diagrams of the ellipsoid significantly\n<!-- , while the ones for the sphere hardly change due to the fact that the eccentricity is constant -->\n\n</div> \n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\nLower bounds extend to Rips filtrations _augmented_ with real-valued functions $f, g$: \n\n$$\nd_B(\\mathcal{R}(\\lambda \\cdot f_X), \\mathcal{R}(\\lambda \\cdot f_Y)) \\leq \\max(1, \\lambda L) \\cdot d_{\\mathrm{GH}}(X, Y)\n$$\n\n:::{layout-ncol=4}\n\n![](images/camel1_rips.png){width=450 height=100% fig-align=\"left\"}\n\n![](images/camel_dgm_rips.png){width=450 height=100% fig-align=\"left\"}\n\n![](images/camel1_ecc.png){width=450 height=100% fig-align=\"left\"}\n\n![](images/camel_dgm_ecc.png){width=450 height=100% fig-align=\"left\"}\n\n:::\n\n<div style=\"text-align: center;\"> \n\nLarger values of $\\lambda$ yield worse bounds, but can lead to simpler diagrams\n\n</div>\n\n<!-- Extra structure combines stability of persistence with flexibility of metrics -->\n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n\n<div style=\"text-align: center;\"> \n\nEach choice of $\\lambda > 0$ yields a _stable signature_ via $\\mathcal{R}(\\lambda \\cdot f_X)$\n\n</div> \n\n<div style=\"text-align: center;\"> \n\nWhich value of $\\lambda$ to choose?\n\n</div>\n\n![](images/camel1_interp1.png){width=950 height=100% fig-align=\"center\"}\n\n\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n<div style=\"text-align: center;\"> \n\nEach choice of $\\lambda > 0$ yields a _stable signature_ via $\\mathcal{R}(\\lambda \\cdot f_X)$\n\n</div> \n\n<div style=\"text-align: center;\"> \n\nWhich value of $\\lambda$ to choose?\n\n</div>\n\n![](images/camel1_interp2.png){width=950 height=100% fig-align=\"center\"}\n\n<div style=\"text-align: center;\"> \n\nWe sample from $\\Delta_+$ randomly, retaining signatures with sufficient topological activity\n\n</div>\n\n## Experiment \\#2: Intrinsic signatures {visibility=\"hidden\"}\n\n...and compared the computed spectral signatures under the relative distance metric: \n\n<!-- \\partial_p^\\ast = U \\Sigma V^T -->\n$$\n\\Lambda(\\mu_p^R) = \\{\\sigma_1, \\sigma_2, \\dots, \\sigma_n \\}, \\quad \\quad \\chi(\\mathbf{\\sigma}, \\mathbf{\\tilde{\\sigma}}) = \\sum\\limits_{i=1}^n \\frac{\\lvert \\sigma_i - \\tilde{\\sigma}_i \\rvert}{\\sqrt{\\sigma_i + \\tilde{\\sigma}_i}}\n$$\n\n![](images/dw_chi_comp.png){width=750 height=100% fig-align=\"center\"}\n\n\n<!-- ## Experiment \\#3: Filtration optimization -->\n\n<!-- ![](images/smoothed_mu.png){width=55% height=100%} -->\n\n\n\n\n\n<!-- __Summary:__ We can obtain $\\mu_p^R(K, f_\\alpha)$ for varying $\\alpha$ by using thresholded versions of $f_\\alpha$ as scalar-products  -->\n\n## Overview {visibility=\"hidden\"}"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"katex","url":"/"},"slide-level":2,"to":"revealjs","filters":["roughnotation"],"css":["katex.min.css","styles.css"],"output-file":"aatrn.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.5.56","auto-stretch":true,"title":"Spectral relaxations of persistent rank invariants","author":"Matt Piekenbrock$\\mathrm{}^\\dagger$   \\&   Jose Perea$\\mathrm{}^\\ddagger$","revealjs-plugins":["spotlight"],"html":{"html-math-method":"katex","standalone":true},"bibliography":["../references.bib"],"defaultTiming":60,"smaller":true,"theme":"simple","institute":["$\\dagger$ Khoury College of Computer Sciences, Northeastern University","$\\ddagger$. Department of Mathematics and Khoury College of Computer Sciences, Northeastern University"],"spotlight":{"useAsPointer":false,"size":55,"toggleSpotlightOnMouseDown":false,"spotlightOnKeyPressAndHold":16,"presentingCursor":"default"},"overview":true,"margin":0.075,"title-slide-attributes":{"data-background-image":"images/NE.jpeg","data-background-size":"contain","data-background-opacity":"0.25"}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["roughnotation"],"output-file":"aatrn.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"title":"Spectral relaxations of persistent rank invariants","author":"Matt Piekenbrock$\\mathrm{}^\\dagger$   \\&   Jose Perea$\\mathrm{}^\\ddagger$","revealjs-plugins":["spotlight"],"html":{"html-math-method":"katex","standalone":true},"bibliography":["../references.bib"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":[]}